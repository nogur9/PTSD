{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "#from tensorflow.keras.layers import Conv2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.python.framework import ops\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "ops.reset_default_graph()\n",
    "random.seed(271828)\n",
    "np.random.seed(271828)\n",
    "#import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "tf.random.set_seed(271828)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multitask-learning-teach-your-ai-more-to-make-it-better-dde116c2cd40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dict = {\n",
    "    'q6.1_INTRU': 1,\n",
    "    'q6.2_DREAM': 1,\n",
    "    'q6.3_FLASH': 1,\n",
    "    'q6.4_UPSET': 0,\n",
    "    'q6.5_PHYS': 0,\n",
    "    'q6.6_AVTHT': 0,\n",
    "    'q6.7_AVSIT': 0,\n",
    "    'q6.8_AMNES': 0,\n",
    "    'q6.9_DISINT': 0,\n",
    "    'q6.10_DTACH': 0,\n",
    "    'q6.11_NUMB': 0,\n",
    "    'q6.12_FUTRE': 0,\n",
    "    'q6.13_SLEEP': 0,\n",
    "    'q6.14_ANGER': 0,\n",
    "    'q6.15_CONC': 0,\n",
    "    'q6.16_HYPER': 0,\n",
    "    'q6.17_STRTL': 0,\n",
    "    'intrusion_cutoff': 1,\n",
    "    'avoidance_cutoff': 0,\n",
    "    'hypertention_cutoff': 0,\n",
    "    'depression_cutoff':0,\n",
    "    'only_avoidance_cutoff': 0,\n",
    "    'regression_cutoff_33': 1,\n",
    "    'regression_cutoff_50': 0,\n",
    "    'tred_cutoff': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH', 'q6.4_UPSET', 'q6.5_PHYS',\n",
    "                 'q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES', 'q6.9_DISINT', 'q6.10_DTACH',\n",
    "                 'q6.11_NUMB', 'q6.12_FUTRE', 'q6.13_SLEEP', 'q6.14_ANGER', 'q6.15_CONC',\n",
    "                 'q6.16_HYPER', 'q6.17_STRTL']\n",
    "targets_list = [i for i in targets_dict if targets_dict[i] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PCL_calculator(df):\n",
    "\n",
    "    symptomatic_cutoff = 2\n",
    "\n",
    "    intrusion = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH', 'q6.4_UPSET', 'q6.5_PHYS']\n",
    "    avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES', 'q6.9_DISINT', 'q6.10_DTACH',\n",
    "                 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "    tred = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH']\n",
    "    only_avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES']\n",
    "\n",
    "    hypertension = ['q6.13_SLEEP', 'q6.14_ANGER', 'q6.15_CONC', 'q6.16_HYPER', 'q6.17_STRTL']\n",
    "\n",
    "    depression = ['q6.9_DISINT', 'q6.10_DTACH', 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "\n",
    "    df[intrusion + avoidance + hypertension].fillna(df[intrusion + avoidance + hypertension].mean(axis=1))\n",
    "    intrusion_cuoff = 1\n",
    "    avoidance_cuoff = 3\n",
    "    hypertension_cuoff = 2\n",
    "    only_avoidance_cutoff = 1\n",
    "    depression_cutoff = 2\n",
    "    tred_cutoff = 1\n",
    "\n",
    "    df['sum'] = (df[intrusion + avoidance + hypertension]).sum(axis=1)\n",
    "\n",
    "    df['intrusion'] = (df[intrusion] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['intrusion_cutoff'] = df['intrusion'] >= intrusion_cuoff\n",
    "\n",
    "    df['avoidance'] = (df[avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['avoidance_cutoff'] = df['avoidance'] >= avoidance_cuoff\n",
    "\n",
    "    df['depression'] = (df[depression] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['depression_cutoff'] = df['depression'] >= depression_cutoff\n",
    "\n",
    "    df['hypertention'] = (df[hypertension] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['hypertention_cutoff'] = df['hypertention'] >= hypertension_cuoff\n",
    "\n",
    "    df['tred'] = (df[tred] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['tred_cutoff'] = df['tred'] >= tred_cutoff\n",
    "\n",
    "    df['only_avoidance'] = (df[only_avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['only_avoidance_cutoff'] = df['only_avoidance'] >= only_avoidance_cutoff\n",
    "\n",
    "    df['regression_cutoff_33'] = df['sum'] >= 33\n",
    "    df['regression_cutoff_50'] = df['sum'] >= 50\n",
    "    df['diagnosis'] = ((df['hypertention_cutoff']) & (df['avoidance_cutoff']) & (df['intrusion_cutoff']) & (df['sum'] >= 50))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PTSD_Model:\n",
    "\n",
    "    features = [\"age\", \"highschool_diploma\", \"dyslexia\", \"ADHD\", \"T1Acc1t\", \"T1Acc1n\", \"T1bias\", \"phq1\", \"lot1\",\n",
    "                \"trait1\",\n",
    "                \"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\", \"phq2\", \"lot2\", \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\",\n",
    "                \"PCL_Strict2\", \"cd_risc1\", \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\",\n",
    "                \"humor1\",\n",
    "                \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\", \"active_coping2\", \"planning2\",\n",
    "                \"positive_reframing2\", \"acceptance2\", \"humor2\", \"religion2\", \"emotional_support2\",\n",
    "                \"instrumental_support2\",\n",
    "                \"self_distraction2\", \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\",\n",
    "                \"self_blame2\",\n",
    "                \"trauma_history8_1\", \"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\", \"COMT_Hap1_recode\",\n",
    "                \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\"]\n",
    "\n",
    "    features_2 = ['q6.1_INTRU_pcl1', 'q6.2_DREAM_pcl1',\n",
    "                  'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                  'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                  'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                  'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1',\n",
    "                  'intrusion_pcl1', 'avoidance_pcl1', 'hypertention_pcl1', 'depression_pcl1', 'tred_pcl1',\n",
    "                  'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2',\n",
    "                  'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2',\n",
    "                  'q6.5_PHYS_pcl2', 'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                  'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                  'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2',\n",
    "                  'intrusion_pcl2', 'avoidance_pcl2', 'hypertention_pcl2', 'depression_pcl2', 'tred_pcl2']\n",
    "    target_features = [\"PCL_Strict3\", \"PCL3\"]\n",
    "    target_features_2 = [\"intrusion_cutoff\", \"avoidance_cutoff\", \"hypertention_cutoff\",\n",
    "                         'depression_cutoff', 'diagnosis', \"PCL3\", \"only_avoidance_cutoff\", \"tred_cutoff\",\n",
    "                         \"regression_cutoff_33\", \"regression_cutoff_50\"]\n",
    "    ID = [\"ID\"]\n",
    "    dataset_path = r\"../Data/PTSD.xlsx\"\n",
    "\n",
    "    multiple_features_no_imputation = ['q6.16_HYPER_pcl1',  'hypertention_pcl2', 'q6.5_PHYS_pcl2', 'q6.12_FUTRE_pcl1',\n",
    "                                       'cd_risc1',  'q6.2_DREAM_pcl2',  'q6.14_ANGER_pcl2', 'positive_reframing2',\n",
    "                                       'venting2', 'q6.15_CONC_pcl1', 'q6.8_AMNES_pcl1',\n",
    "                                       'q6.15_CONC_pcl2', 'PCL_Broad2', 'phq2', 'q6.4_UPSET_pcl2']\n",
    "\n",
    "    def __init__(self):\n",
    "        path = \"C:\\‏‏PycharmProjects\\PTSD\\Data\\PTSD.xlsx\"\n",
    "        df = pd.read_excel(path)\n",
    "        df = df[~df['PCL_Strict3'].isna()]\n",
    "        #df = df[~ ((df[\"military_exp18_t3\"] == 0) & (df[\"military_exp18_t2\"] == 0))]\n",
    "        df = df[self.features + self.ID + self.target_features]\n",
    "        df_pcl3 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL3.xlsx\")\n",
    "        df_pcl3 = PCL_calculator(df_pcl3)\n",
    "        df_pcl2 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL2.xlsx\")\n",
    "        df_pcl2 = PCL_calculator(df_pcl2)\n",
    "        df_pcl1 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL1.xlsx\")\n",
    "        df_pcl1 = PCL_calculator(df_pcl1)\n",
    "\n",
    "        df = df.merge(df_pcl1, on=\"ID\", how='outer')\n",
    "        df = df.merge(df_pcl2, suffixes=('_pcl1', '_pcl2'), on=\"ID\", how='outer')\n",
    "        df = df.merge(df_pcl3.drop(['PCL3_Strict', 'pcl3', 'PCL3_Broad'], axis=1), on=\"ID\", how='outer')\n",
    "\n",
    "        df = df[~df['PCL_Strict3'].isna()]\n",
    "        #df = df[~df['tred_cutoff'].isna()]\n",
    "        df.drop(self.ID, inplace=True, axis=1)\n",
    "        mice = IterativeImputer()\n",
    "        df = pd.DataFrame(mice.fit_transform(df), columns=df.columns)\n",
    "\n",
    "        all_x_col = self.features + self.features_2 + self.target_features_2 + questions\n",
    "        #all_x_col = self.features + self.features_2\n",
    "        #y_col = [\"tred_cutoff\"]\n",
    "        y_col = [\"PCL_Strict3\"]\n",
    "        X = df[all_x_col]\n",
    "        Y = df[y_col]\n",
    "        X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X, Y, test_size=0.25, random_state=271828, stratify=Y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_0, y_train_0, test_size=0.25, random_state=271828, stratify=y_train_0)\n",
    "        df = pd.concat([X_train, y_train], axis=1)\n",
    "        self.X_test = X_test\n",
    "        self.y_test =y_test\n",
    "\n",
    "        self.X_train_0 = X_train_0\n",
    "        self.X_test_0 = X_test_0\n",
    "        self.y_train_0 = y_train_0\n",
    "        self.y_test_0 = y_test_0\n",
    "\n",
    "        self.df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PTSD_Model()\n",
    "X = m.df.drop(\"PCL_Strict3\", axis=1)\n",
    "Y01 = m.df[\"PCL_Strict3\"].apply(lambda x: int(x))\n",
    "\n",
    "features = m.features + m.features_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lim in [0.5]:\n",
    "    print(f\"____________ Limit = {lim} ______________\")\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    train_scores_f = []\n",
    "    train_scores_p = []\n",
    "    train_scores_r = []\n",
    "\n",
    "    scores_f = []\n",
    "    scores_p = []\n",
    "    scores_r = []\n",
    "    for train, test in kfold.split(X, Y01):\n",
    "        \n",
    "        Y1 = X['tred_cutoff'].apply(lambda x: int(x > 0))\n",
    "        X_train = X.iloc[train]\n",
    "        y_train_1 = X_train['tred_cutoff'].apply(lambda x: int(x > 0))\n",
    "       \n",
    "        y_trains = [y_train_1]\n",
    "        for i in targets_list:\n",
    "            y_trains.append(X_train[i].apply(lambda x: int(x > 1)))\n",
    "\n",
    "        X_train = X_train[features]\n",
    "\n",
    "        visible = Input(shape=(102,))\n",
    "\n",
    "        x = Dense(10, activation='elu')(visible)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        x = Dense(10, activation='elu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        x = Dense(10, activation='elu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        outputs = [Dense(1, activation='sigmoid')(x)]\n",
    "        \n",
    "        for i in targets_list:\n",
    "            outputs.append(Dense(1, activation='sigmoid')(x))\n",
    "        \n",
    "        \n",
    "        model = Model(inputs=visible, outputs=outputs)\n",
    "        \n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=25)]\n",
    "        model.compile(loss=['binary_crossentropy']+['binary_crossentropy'] * len(targets_list),\n",
    "              optimizer=Adam(), loss_weights =  ([1] + [1] * len(targets_list)))\n",
    "        \n",
    "        model.fit(X_train,y_trains , epochs = 400, verbose=0,\n",
    "                 class_weight= [{1:3, 0:1}] + [{0:1, 1:1}] * len(targets_list)\n",
    "                 )\n",
    "        # evaluate the model\n",
    "        y_pred = (model.predict(X[features].iloc[test])[0] > lim).astype(int)\n",
    "\n",
    "        s_f = f1_score(Y1.iloc[test], y_pred)\n",
    "        s_p = precision_score(Y1.iloc[test], y_pred)\n",
    "        s_r = recall_score(Y1.iloc[test], y_pred)\n",
    "        print(f\"sum(Y1.iloc[test]) = {sum(Y1.iloc[test])}\\nsum(y_pred)={sum(y_pred)}\")\n",
    "        print(\"\\tscores f1\", (s_f))\n",
    "        print(\"\\tscores p\", (s_p))\n",
    "        print(\"\\tscores r\", (s_r))\n",
    "        scores_f.append(s_f)\n",
    "        scores_p.append(s_p)\n",
    "        scores_r.append(s_r)\n",
    "\n",
    "    print(\"mean scores f1\", np.mean(scores_f))\n",
    "    print(\"mean scores p\", np.mean(scores_p))\n",
    "    print(\"mean scores r\", np.mean(scores_r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________ Limit = 0.5 ______________\n",
      "sum(Y1.iloc[test]) = 8\n",
      "sum(y_pred)=[23]\n",
      "\tscores f1 0.3870967741935483\n",
      "\tscores p 0.2608695652173913\n",
      "\tscores r 0.75\n",
      "sum(Y1.iloc[test]) = 6\n",
      "sum(y_pred)=[11]\n",
      "\tscores f1 0.4705882352941177\n",
      "\tscores p 0.36363636363636365\n",
      "\tscores r 0.6666666666666666\n",
      "sum(Y1.iloc[test]) = 7\n",
      "sum(y_pred)=[3]\n",
      "\tscores f1 0.4\n",
      "\tscores p 0.6666666666666666\n",
      "\tscores r 0.2857142857142857\n",
      "sum(Y1.iloc[test]) = 9\n",
      "sum(y_pred)=[9]\n",
      "\tscores f1 0.6666666666666666\n",
      "\tscores p 0.6666666666666666\n",
      "\tscores r 0.6666666666666666\n",
      "sum(Y1.iloc[test]) = 12\n",
      "sum(y_pred)=[12]\n",
      "\tscores f1 0.5\n",
      "\tscores p 0.5\n",
      "\tscores r 0.5\n",
      "sum(Y1.iloc[test]) = 2\n",
      "sum(y_pred)=[12]\n",
      "\tscores f1 0.2857142857142857\n",
      "\tscores p 0.16666666666666666\n",
      "\tscores r 1.0\n",
      "sum(Y1.iloc[test]) = 3\n",
      "sum(y_pred)=[12]\n",
      "\tscores f1 0.26666666666666666\n",
      "\tscores p 0.16666666666666666\n",
      "\tscores r 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "for lim in [0.5]:\n",
    "    print(f\"____________ Limit = {lim} ______________\")\n",
    "    kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "    train_scores_f = []\n",
    "    train_scores_p = []\n",
    "    train_scores_r = []\n",
    "\n",
    "    scores_f = []\n",
    "    scores_p = []\n",
    "    scores_r = []\n",
    "    for train, test in kfold.split(X, Y01):\n",
    "        \n",
    "        Y1 = X['tred_cutoff'].apply(lambda x: int(x > 0))\n",
    "        X_train = X.iloc[train]\n",
    "        y_train_1 = X_train['tred_cutoff'].apply(lambda x: int(x > 0))\n",
    "        sm = SMOTE(random_state=27)\n",
    "        cols = X_train.columns\n",
    "        X_train, y_train_1 = sm.fit_sample(X_train, y_train_1.ravel())\n",
    "        X_train = pd.DataFrame(X_train, columns=cols)\n",
    "        y_trains = [y_train_1]\n",
    "        \n",
    "        for i in targets_list:\n",
    "            y_trains.append(X_train[i].apply(lambda x: int(x > 0)))\n",
    "\n",
    "        X_train = X_train[features]\n",
    "\n",
    "        visible = Input(shape=(102,))\n",
    "     \n",
    "\n",
    "        x = Dense(10, activation='elu')(visible)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        x = Dense(10, activation='elu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "        x = Dense(10, activation='elu')(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "        outputs = [Dense(1, activation='sigmoid')(x)]\n",
    "        \n",
    "        for i in targets_list:\n",
    "            outputs.append(Dense(1, activation='sigmoid')(x))\n",
    "        \n",
    "        \n",
    "        model = Model(inputs=visible, outputs=outputs)\n",
    "        #callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "        model.compile(loss=['binary_crossentropy']+['binary_crossentropy'] * len(targets_list),\n",
    "              optimizer=Adam(), loss_weights =  ([1] + [1] * len(targets_list)))\n",
    "        \n",
    "        model.fit(X_train,y_trains , epochs = 400, verbose=0)#, callbacks=callbacks)\n",
    "        \n",
    "        y_pred = (model.predict(X[features].iloc[test])[0] > lim).astype(int)\n",
    "\n",
    "        s_f = f1_score(Y1.iloc[test], y_pred)\n",
    "        s_p = precision_score(Y1.iloc[test], y_pred)\n",
    "        s_r = recall_score(Y1.iloc[test], y_pred)\n",
    "        print(f\"sum(Y1.iloc[test]) = {sum(Y1.iloc[test])}\\nsum(y_pred)={sum(y_pred)}\")\n",
    "        print(\"\\tscores f1\", (s_f))\n",
    "        print(\"\\tscores p\", (s_p))\n",
    "        print(\"\\tscores r\", (s_r))\n",
    "        scores_f.append(s_f)\n",
    "        scores_p.append(s_p)\n",
    "        scores_r.append(s_r)\n",
    "\n",
    "    print(\"mean scores f1\", np.mean(scores_f))\n",
    "    print(\"mean scores p\", np.mean(scores_p))\n",
    "    print(\"mean scores r\", np.mean(scores_r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(X['tred_cutoff']) / (len(X['tred_cutoff']) - sum(X['tred_cutoff']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(Y01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

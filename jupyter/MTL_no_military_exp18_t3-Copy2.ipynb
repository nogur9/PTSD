{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "#from tensorflow.keras.layers import Conv2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fancyimpute import IterativeImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "ops.reset_default_graph()\n",
    "random.seed(271828)\n",
    "np.random.seed(271828)\n",
    "#import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "tf.random.set_seed(271828)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/multitask-learning-teach-your-ai-more-to-make-it-better-dde116c2cd40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_dict = {\n",
    "    'q6.1_INTRU': 1,\n",
    "    'q6.2_DREAM': 1,\n",
    "    'q6.3_FLASH': 1,\n",
    "    'q6.4_UPSET': 1,\n",
    "    'q6.5_PHYS': 1,\n",
    "    'q6.6_AVTHT': 1,\n",
    "    'q6.7_AVSIT': 1,\n",
    "    'q6.8_AMNES': 1,\n",
    "    'q6.9_DISINT': 1,\n",
    "    'q6.10_DTACH': 1,\n",
    "    'q6.11_NUMB': 1,\n",
    "    'q6.12_FUTRE': 1,\n",
    "    'q6.13_SLEEP': 1,\n",
    "    'q6.14_ANGER': 1,\n",
    "    'q6.15_CONC': 1,\n",
    "    'q6.16_HYPER': 1,\n",
    "    'q6.17_STRTL': 1,\n",
    "    'intrusion_cutoff': 0,\n",
    "    'avoidance_cutoff': 0,\n",
    "    'hypertention_cutoff': 0,\n",
    "    'depression_cutoff': 0,\n",
    "    'only_avoidance_cutoff': 0,\n",
    "    'regression_cutoff_33': 0,\n",
    "    'regression_cutoff_50': 0,\n",
    "    'tred_cutoff': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH', 'q6.4_UPSET', 'q6.5_PHYS',\n",
    "                 'q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES', 'q6.9_DISINT', 'q6.10_DTACH',\n",
    "                 'q6.11_NUMB', 'q6.12_FUTRE', 'q6.13_SLEEP', 'q6.14_ANGER', 'q6.15_CONC',\n",
    "                 'q6.16_HYPER', 'q6.17_STRTL']\n",
    "targets_list = [i for i in targets_dict if targets_dict[i] == 1]\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PCL_calculator(df):\n",
    "\n",
    "    symptomatic_cutoff = 2\n",
    "\n",
    "    intrusion = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH', 'q6.4_UPSET', 'q6.5_PHYS']\n",
    "    avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES', 'q6.9_DISINT', 'q6.10_DTACH',\n",
    "                 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "    tred = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH']\n",
    "    only_avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES']\n",
    "\n",
    "    hypertension = ['q6.13_SLEEP', 'q6.14_ANGER', 'q6.15_CONC', 'q6.16_HYPER', 'q6.17_STRTL']\n",
    "\n",
    "    depression = ['q6.9_DISINT', 'q6.10_DTACH', 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "\n",
    "    df[intrusion + avoidance + hypertension].fillna(df[intrusion + avoidance + hypertension].mean(axis=1))\n",
    "    intrusion_cuoff = 1\n",
    "    avoidance_cuoff = 3\n",
    "    hypertension_cuoff = 2\n",
    "    only_avoidance_cutoff = 1\n",
    "    depression_cutoff = 2\n",
    "    tred_cutoff = 1\n",
    "\n",
    "    df['sum'] = (df[intrusion + avoidance + hypertension]).sum(axis=1)\n",
    "\n",
    "    df['intrusion'] = (df[intrusion] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['intrusion_cutoff'] = df['intrusion'] >= intrusion_cuoff\n",
    "\n",
    "    df['avoidance'] = (df[avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['avoidance_cutoff'] = df['avoidance'] >= avoidance_cuoff\n",
    "\n",
    "    df['depression'] = (df[depression] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['depression_cutoff'] = df['depression'] >= depression_cutoff\n",
    "\n",
    "    df['hypertention'] = (df[hypertension] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['hypertention_cutoff'] = df['hypertention'] >= hypertension_cuoff\n",
    "\n",
    "    df['tred'] = (df[tred] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['tred_cutoff'] = df['tred'] >= tred_cutoff\n",
    "\n",
    "    df['only_avoidance'] = (df[only_avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['only_avoidance_cutoff'] = df['only_avoidance'] >= only_avoidance_cutoff\n",
    "\n",
    "    df['regression_cutoff_33'] = df['sum'] >= 33\n",
    "    df['regression_cutoff_50'] = df['sum'] >= 50\n",
    "    df['diagnosis'] = ((df['hypertention_cutoff']) & (df['avoidance_cutoff']) & (df['intrusion_cutoff']) & (df['sum'] >= 50))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PTSD_Model:\n",
    "\n",
    "    features = [\"age\", \"highschool_diploma\", \"dyslexia\", \"ADHD\", \"T1Acc1t\", \"T1Acc1n\", \"T1bias\", \"phq1\", \"lot1\",\n",
    "                \"trait1\",\n",
    "                \"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\", \"phq2\", \"lot2\", \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\",\n",
    "                \"PCL_Strict2\", \"cd_risc1\", \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\",\n",
    "                \"humor1\",\n",
    "                \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\", \"active_coping2\", \"planning2\",\n",
    "                \"positive_reframing2\", \"acceptance2\", \"humor2\", \"religion2\", \"emotional_support2\",\n",
    "                \"instrumental_support2\",\n",
    "                \"self_distraction2\", \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\",\n",
    "                \"self_blame2\",\n",
    "                \"trauma_history8_1\", \"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\", \"COMT_Hap1_recode\",\n",
    "                \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\"]\n",
    "\n",
    "    features_2 = ['q6.1_INTRU_pcl1', 'q6.2_DREAM_pcl1',\n",
    "                  'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                  'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                  'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                  'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1',\n",
    "                  'intrusion_pcl1', 'avoidance_pcl1', 'hypertention_pcl1', 'depression_pcl1', 'tred_pcl1',\n",
    "                  'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2',\n",
    "                  'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2',\n",
    "                  'q6.5_PHYS_pcl2', 'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                  'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                  'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2',\n",
    "                  'intrusion_pcl2', 'avoidance_pcl2', 'hypertention_pcl2', 'depression_pcl2', 'tred_pcl2']\n",
    "    target_features = [\"PCL_Strict3\", \"PCL3\"]\n",
    "    target_features_2 = [\"intrusion_cutoff\", \"avoidance_cutoff\", \"hypertention_cutoff\",\n",
    "                         'depression_cutoff', 'diagnosis', \"PCL3\", \"only_avoidance_cutoff\", \"tred_cutoff\",\n",
    "                         \"regression_cutoff_33\", \"regression_cutoff_50\"]\n",
    "    ID = [\"ID\"]\n",
    "    dataset_path = r\"../Data/PTSD.xlsx\"\n",
    "\n",
    "    multiple_features_no_imputation = ['q6.16_HYPER_pcl1',  'hypertention_pcl2', 'q6.5_PHYS_pcl2', 'q6.12_FUTRE_pcl1',\n",
    "                                       'cd_risc1',  'q6.2_DREAM_pcl2',  'q6.14_ANGER_pcl2', 'positive_reframing2',\n",
    "                                       'venting2', 'q6.15_CONC_pcl1', 'q6.8_AMNES_pcl1',\n",
    "                                       'q6.15_CONC_pcl2', 'PCL_Broad2', 'phq2', 'q6.4_UPSET_pcl2']\n",
    "\n",
    "    def __init__(self):\n",
    "        path = \"C:\\‏‏PycharmProjects\\PTSD\\Data\\PTSD.xlsx\"\n",
    "        df = pd.read_excel(path)\n",
    "        df = df[~df['PCL_Strict3'].isna()]\n",
    "        #df = df[df[\"military_exp18_t3\"] > 0]\n",
    "        df = df[self.features + self.ID + self.target_features]\n",
    "        df_pcl3 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL3.xlsx\")\n",
    "        df_pcl3 = PCL_calculator(df_pcl3)\n",
    "        df_pcl2 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL2.xlsx\")\n",
    "        df_pcl2 = PCL_calculator(df_pcl2)\n",
    "        df_pcl1 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL1.xlsx\")\n",
    "        df_pcl1 = PCL_calculator(df_pcl1)\n",
    "\n",
    "        df = df.merge(df_pcl1, on=\"ID\", how='outer')\n",
    "        df = df.merge(df_pcl2, suffixes=('_pcl1', '_pcl2'), on=\"ID\", how='outer')\n",
    "        df = df.merge(df_pcl3.drop(['PCL3_Strict', 'pcl3', 'PCL3_Broad'], axis=1), on=\"ID\", how='outer')\n",
    "\n",
    "        df = df[~df['PCL_Strict3'].isna()]\n",
    "        #df = df[~df['tred_cutoff'].isna()]\n",
    "        df.drop(self.ID, inplace=True, axis=1)\n",
    "        mice = IterativeImputer()\n",
    "        df = pd.DataFrame(mice.fit_transform(df), columns=df.columns)\n",
    "\n",
    "        all_x_col = self.features + self.features_2 + self.target_features_2 + questions\n",
    "        #all_x_col = self.features + self.features_2\n",
    "        #y_col = [\"tred_cutoff\"]\n",
    "        y_col = [\"PCL_Strict3\"]\n",
    "        X = df[all_x_col]\n",
    "        Y = df[y_col]\n",
    "        X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(X, Y, test_size=0.25, random_state=271828, stratify=Y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_train_0, y_train_0, test_size=0.25, random_state=271828, stratify=y_train_0)\n",
    "        df = pd.concat([X_train, y_train], axis=1)\n",
    "        self.X_test = X_test\n",
    "        self.y_test =y_test\n",
    "\n",
    "        self.X_train_0 = X_train_0\n",
    "        self.X_test_0 = X_test_0\n",
    "        self.y_train_0 = y_train_0\n",
    "        self.y_test_0 = y_test_0\n",
    "\n",
    "        self.df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PTSD_Model()\n",
    "X = m.df.drop(\"PCL_Strict3\", axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m.df[\"PCL_Strict3\"].apply(lambda x: int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y01 = m.df[\"PCL_Strict3\"].apply(lambda x: int(x))\n",
    "\n",
    "features = m.features + m.features_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train_1, y_test_1, y_train_2, y_test_2, y_train_3, y_test_3, y_train_4, y_test_4, \\\n",
    "# y_train_5, y_test_5, y_train_6, y_test_6, y_train_7, y_test_7, y_train_8, y_test_8, y_train_9, y_test_9, \\\n",
    "# y_train_10, y_test_10, y_train_11, y_test_11, y_train_12, y_test_12, y_train_13, y_test_13, y_train_14, y_test_14, \\\n",
    "# y_train_15, y_test_15, y_train_16, y_test_16, y_train_17, y_test_17, y_train_18, y_test_18 = \\\n",
    "# train_test_split(X[features], Y1, y[0], y[1], y[2], y[3], y[4], y[5], y[6], y[7], \\\n",
    "#                  y[8], y[9], y[10], y[11], y[12], y[13], y[14], y[15], y[16], stratify=Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum(Y1)/(len(Y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visible = Input(shape=(102,))\n",
    "# x = Dense(20, activation='relu')(visible)\n",
    "# #x = Dropout(0.5)(x)\n",
    "# x = Dense(10, activation='relu')(x)\n",
    "# #x = Dropout(0.5)(x)\n",
    "# # x = Dense(7, activation='relu')(x)\n",
    "# # x = Dropout(0.5)(x)\n",
    "\n",
    "# output1 = Dense(1, activation='sigmoid')(x)\n",
    "# output2 = Dense(1, activation='sigmoid')(x)\n",
    "# output3 = Dense(1, activation='sigmoid')(x)\n",
    "# output4 = Dense(1, activation='sigmoid')(x)\n",
    "# output5 = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# output6 = Dense(1, activation='sigmoid')(x)\n",
    "# output7 = Dense(1, activation='sigmoid')(x)\n",
    "# output8 = Dense(1, activation='sigmoid')(x)\n",
    "# output9 = Dense(1, activation='sigmoid')(x)\n",
    "# output10 = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# output11 = Dense(1, activation='sigmoid')(x)\n",
    "# output12 = Dense(1, activation='sigmoid')(x)\n",
    "# output13 = Dense(1, activation='sigmoid')(x)\n",
    "# output14 = Dense(1, activation='sigmoid')(x)\n",
    "# output15 = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# output16 = Dense(1, activation='sigmoid')(x)\n",
    "# output17 = Dense(1, activation='sigmoid')(x)\n",
    "# output18 = Dense(1, activation='sigmoid')(x)\n",
    "# model = Model(inputs=visible, outputs=[output1, output2, output3, output4, output5, \n",
    "#                                       output6, output7, output8, output9, output10, \n",
    "#                                       output11, output12, output13, output14, output15,\n",
    "#                                       output16, output17, output18])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(loss=['binary_crossentropy', 'mean_squared_error', 'mean_squared_error', 'mean_squared_error', 'mean_squared_error',\n",
    "#                    'mean_squared_error', 'mean_squared_error', 'mean_squared_error', 'mean_squared_error',\n",
    "#                    'mean_squared_error', 'mean_squared_error', 'mean_squared_error', 'mean_squared_error',\n",
    "#                    'mean_squared_error', 'mean_squared_error', 'mean_squared_error', 'mean_squared_error', 'mean_squared_error'],\n",
    "#               optimizer=Adam(), loss_weights = [1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1,\n",
    "#                                                1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1, 1e-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, [y_train_1, y_train_2, y_train_3, y_train_4, y_train_5,\n",
    "#                    y_train_6, y_train_7, y_train_8, y_train_9, y_train_10,\n",
    "#                    y_train_11, y_train_12, y_train_13, y_train_14, y_train_15,\n",
    "#                    y_train_16, y_train_17, y_train_18], epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "# precision_score(y_test_1, model.predict(X_test)[0]>0.4)\n",
    "#f1_score(y_test_1, model.predict(X_test)[0]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________ Limit = 0.5 ______________\n",
      "sum(Y1.iloc[test]) = 5\n",
      "sum(y_pred)=[4]\n",
      "\tscores f1 0.22222222222222224\n",
      "\tscores p 0.25\n",
      "\tscores r 0.2\n",
      "sum(Y1.iloc[test]) = 5\n",
      "sum(y_pred)=[2]\n",
      "\tscores f1 0.28571428571428575\n",
      "\tscores p 0.5\n",
      "\tscores r 0.2\n",
      "sum(Y1.iloc[test]) = 5\n",
      "sum(y_pred)=[9]\n",
      "\tscores f1 0.2857142857142857\n",
      "\tscores p 0.2222222222222222\n",
      "\tscores r 0.4\n",
      "sum(Y1.iloc[test]) = 5\n",
      "sum(y_pred)=[7]\n",
      "\tscores f1 0.5\n",
      "\tscores p 0.42857142857142855\n",
      "\tscores r 0.6\n",
      "sum(Y1.iloc[test]) = 5\n",
      "sum(y_pred)=[8]\n",
      "\tscores f1 0.4615384615384615\n",
      "\tscores p 0.375\n",
      "\tscores r 0.6\n",
      "mean scores f1 0.351037851037851\n",
      "mean scores p 0.3551587301587301\n",
      "mean scores r 0.4\n",
      "____________ Limit = 0.75 ______________\n",
      "sum(Y1.iloc[test]) = 5\n",
      "sum(y_pred)=[2]\n",
      "\tscores f1 0.0\n",
      "\tscores p 0.0\n",
      "\tscores r 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-05e12c777beb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m               optimizer=Adam(), loss_weights = [1] + [1e-1] * len(targets_list))\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_trains\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[1;31m# evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mlim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 873\u001b[1;33m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    334\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    526\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    526\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lim in [0.5, 0.75]:\n",
    "    print(f\"____________ Limit = {lim} ______________\")\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "    train_scores_f = []\n",
    "    train_scores_p = []\n",
    "    train_scores_r = []\n",
    "\n",
    "    scores_f = []\n",
    "    scores_p = []\n",
    "    scores_r = []\n",
    "    for train, test in kfold.split(X, Y01):\n",
    "        \n",
    "        Y1 = m.df[\"PCL_Strict3\"].apply(lambda x: int(x))\n",
    "        X_train = X.iloc[train]\n",
    "        y_train_1 = Y1.iloc[train]\n",
    "        sm = SMOTE(random_state=27)\n",
    "        cols = X_train.columns\n",
    "        X_train, y_train_1 = sm.fit_sample(X_train, y_train_1.ravel())\n",
    "        X_train = pd.DataFrame(X_train, columns=cols)\n",
    "        y_trains = [y_train_1]\n",
    "        for i in targets_list:\n",
    "            y_trains.append(X_train[i].apply(lambda x: int(x > 0)))\n",
    "        \n",
    "        X_train = X_train[features]\n",
    "\n",
    "        visible = Input(shape=(102,))\n",
    "        #x = Dense(30, activation='relu')(visible)\n",
    "        #x = Dropout(0.5)(x)\n",
    "        #x = Dropout(0.1)(x)\n",
    "\n",
    "        x = Dense(10, activation='relu')(visible)\n",
    "        #x = Dropout(0.5)(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        \n",
    "        outputs = [Dense(1, activation='sigmoid')(x)]\n",
    "        \n",
    "        for i in targets_list:\n",
    "            outputs.append(Dense(1, activation='sigmoid')(x))\n",
    "    \n",
    "\n",
    "        #outputs = [output1, output2, output3, output4, output5, output6,\n",
    "         #          output7, output8, output9, output10, output11,\n",
    "          #         output12, output13, output14, output15, output16,\n",
    "           #        output17, output18, output19, output20, output21,\n",
    "            #       output22, output23, output24, output25, output26]\n",
    "        \n",
    "        \n",
    "        model = Model(inputs=visible, outputs=outputs)\n",
    "        #callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n",
    "        model.compile(loss=['binary_crossentropy']+['binary_crossentropy'] * len(targets_list),\n",
    "              optimizer=Adam(), loss_weights = [1] + [1e-1] * len(targets_list))\n",
    "        \n",
    "        model.fit(X_train,y_trains , epochs = 400, verbose=0)\n",
    "        # evaluate the model\n",
    "        y_pred = (model.predict(X[features].iloc[test])[0] > lim).astype(int)\n",
    "\n",
    "        s_f = f1_score(Y1.iloc[test], y_pred)\n",
    "        s_p = precision_score(Y1.iloc[test], y_pred)\n",
    "        s_r = recall_score(Y1.iloc[test], y_pred)\n",
    "        print(f\"sum(Y1.iloc[test]) = {sum(Y1.iloc[test])}\\nsum(y_pred)={sum(y_pred)}\")\n",
    "        print(\"\\tscores f1\", (s_f))\n",
    "        print(\"\\tscores p\", (s_p))\n",
    "        print(\"\\tscores r\", (s_r))\n",
    "        scores_f.append(s_f)\n",
    "        scores_p.append(s_p)\n",
    "        scores_r.append(s_r)\n",
    "\n",
    "    print(\"mean scores f1\", np.mean(scores_f))\n",
    "    print(\"mean scores p\", np.mean(scores_p))\n",
    "    print(\"mean scores r\", np.mean(scores_r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Y1[~Y1.isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X[features].iloc[test])[0] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

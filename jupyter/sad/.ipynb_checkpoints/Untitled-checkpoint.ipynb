{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_0.xlsx\n",
      "\n",
      "Social Anxiety LSAS cutoff is 50\n",
      "\n",
      "pos y = 70\n",
      "neg y = 61\n",
      "checking generalization with StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
      "score function is accuracy\n",
      "Performing model optimizations...\n",
      "\n",
      "Estimator: xgb\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ab13e1c1a64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nEstimator: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgrid_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# Fit grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 \u001b[1;31m# Best params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best params: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\imblearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    711\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 713\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[0;32m    214\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m-> 1110\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# load df\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.feature_selection import RFE, SelectKBest, SelectFdr, SelectFpr\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, PowerTransformer\n",
    "from imblearn.ensemble import BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV, LeaveOneOut\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "\n",
    "for path in [r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_0.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_1.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_0.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_1.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_0_std_0_ver2.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_0_std_1_ver2.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_1_std_0_ver2.xlsx\",\n",
    "             r\"C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_1_std_1_ver2.xlsx\"]:\n",
    "    print(path)\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    # impute zeros\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # split to holdouts and training\n",
    "    sad_cutoff = 50\n",
    "    print(f\"\\nSocial Anxiety LSAS cutoff is {sad_cutoff}\\n\")\n",
    "\n",
    "    Y = df[\"LSAS\"] >= sad_cutoff\n",
    "    print(f\"pos y = {sum(Y)}\\nneg y = {(len(Y) - sum(Y))}\")\n",
    "\n",
    "    X = df.drop([\"LSAS\",\"Subject\"], axis=1)\n",
    "    c = ((len(Y) - sum(Y)) / sum(Y))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y)\n",
    "\n",
    "\n",
    "    pipe_svc = Pipeline(steps=[\n",
    "                ('scaling', StandardScaler()),\n",
    "                ('classifier', SVC())\n",
    "    ])\n",
    "\n",
    "    params_svc = {\n",
    "            'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'classifier__C': [0.001, 0.01, 1, 10, 100, 1000],\n",
    "            'classifier__gamma': [1, 0.1, 0.001, 0.0001]\n",
    "        }\n",
    "\n",
    "\n",
    "    pipe_rf = Pipeline(steps=[\n",
    "            ('classifier', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    params_rf = {\n",
    "            'classifier__n_estimators': [100, 250, 400, 700],\n",
    "            'classifier__max_depth': [2, 3, 6, 10],\n",
    "            'classifier__max_features': [1.0, 0.8, 0.5, 'auto'],\n",
    "            'classifier__min_samples_split': [1.0, 0.8, 0.5]\n",
    "    }\n",
    "\n",
    "    pipe_bbc = Pipeline(steps=[\n",
    "                ('classifier', BalancedBaggingClassifier())\n",
    "        ])\n",
    "\n",
    "    params_bbc = {\n",
    "                'classifier': [BalancedRandomForestClassifier(), BalancedBaggingClassifier()],\n",
    "                'classifier__n_estimators': [100, 250, 400, 700],\n",
    "                'classifier__max_features': [1.0, 0.9, 0.8, 0.5],\n",
    "    }\n",
    "\n",
    "    pipe_xgb = Pipeline(steps=[\n",
    "                ('classifier', XGBClassifier())\n",
    "    ])\n",
    "\n",
    "    params_xgb = {\n",
    "                'classifier__n_estimators': [100, 300],\n",
    "                'classifier__max_depth': [2, 3, 5],\n",
    "                'classifier__learning_rate': [0.1, 0.05, 0.25],\n",
    "                'classifier__gamma': [0, 0.5, 1],\n",
    "                'classifier__min_child_weight': [1, 0.5, 2],\n",
    "                'classifier__scale_pos_weight': [c, 2 * c, 0.5 * c, 1],\n",
    "                'classifier__reg_alpha': [0, 1, 0.5],\n",
    "                'classifier__reg_lambda': [0, 1, 0.5]\n",
    "        }\n",
    "\n",
    "    pipe_lr = Pipeline(steps=[\n",
    "                ('scaling', StandardScaler()),\n",
    "                ('classifier', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    params_lr = {\n",
    "            'classifier__penalty': ['l1', 'l2'],\n",
    "            'classifier__tol': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "            'classifier__C': [0.0001, 0.001, 0.01,0.05, 0.1,0.5, 1, 10, 100, 1000, 10000],\n",
    "            'classifier__solver': ['liblinear', 'saga', 'warn']\n",
    "    }\n",
    "\n",
    "    pipe_lr2 = Pipeline(steps=[\n",
    "                ('scaling', StandardScaler()),\n",
    "                ('classifier', LogisticRegression(solver='liblinear', dual=True,penalty='l2'))\n",
    "    ])\n",
    "\n",
    "    params_lr2 = {\n",
    "            'classifier__tol': [0.00001, 0.0001, 0.001, 0.01, 0.1],\n",
    "            'classifier__C': [0.0001, 0.001, 0.01,0.05, 0.1,0.5, 1, 10, 100, 1000, 10000],\n",
    "    }\n",
    "\n",
    "    pipe_en = Pipeline(steps=[\n",
    "                ('scaling', StandardScaler()),\n",
    "                ('classifier', SGDClassifier(penalty='elasticnet', loss='log'))\n",
    "    ])\n",
    "    params_en = {\n",
    "            'classifier__alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "            'classifier__tol': [0.000001, 0.00001, 0.0001, 0.001, 0.01],\n",
    "            'classifier__n_iter':[2,3,4,5,6,7,8,9,10],\n",
    "            'classifier__l1_ratio': [0, 0.025, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1],\n",
    "    }\n",
    "    for loo in [StratifiedKFold(5)]:#:, StratifiedKFold(5)]:\n",
    "        for score in ['accuracy']:#, 'f1']:\n",
    "            print(f\"checking generalization with {loo}\\nscore function is {score}\")\n",
    "            gs_svc = GridSearchCV(estimator=pipe_svc,\n",
    "                                  param_grid=params_svc,\n",
    "                                  scoring=score,\n",
    "                                  cv=loo)\n",
    "\n",
    "            gs_rf = GridSearchCV(estimator=pipe_rf,\n",
    "                                 param_grid=params_rf,\n",
    "                                 scoring=score,\n",
    "                                 cv=loo)\n",
    "\n",
    "            gs_bbc = GridSearchCV(estimator=pipe_bbc,\n",
    "                                  param_grid=params_bbc,\n",
    "                                  scoring=score,\n",
    "                                  cv=loo)\n",
    "\n",
    "            gs_xgb = GridSearchCV(estimator=pipe_xgb,\n",
    "                                  param_grid=params_xgb,\n",
    "                                  scoring=score,\n",
    "                                  cv=loo)\n",
    "\n",
    "            gs_lr = GridSearchCV(estimator=pipe_lr,\n",
    "                                 param_grid=params_lr,\n",
    "                                 scoring=score,\n",
    "                                 cv=loo)\n",
    "            gs_lr2 = GridSearchCV(estimator=pipe_lr2,\n",
    "                                 param_grid=params_lr2,\n",
    "                                 scoring=score,\n",
    "                                 cv=loo)\n",
    "            gs_en = GridSearchCV(estimator=pipe_en,\n",
    "                                 param_grid=params_en,\n",
    "                                 scoring=score,\n",
    "                                 cv=loo)\n",
    "\n",
    "\n",
    "            # List of pipelines for ease of iteration\n",
    "            grids = [gs_xgb]#[gs_en, gs_lr, gs_lr2, gs_xgb, gs_rf, gs_bbc, gs_svc, gs_lr]\n",
    "\n",
    "            # Dictionary of pipelines and classifier types for ease of reference\n",
    "            grid_dict = {0: 'xgb'}\n",
    "            #grid_dict = {0: 'en', 1: 'lr', 2: 'lr dual', 3: 'xgb short', 4: 'rf', 5: 'bbc', 6: 'svc', 7: 'lr'}\n",
    "\n",
    "            # Fit the grid search objects\n",
    "            print('Performing model optimizations...')\n",
    "            best_clf = 0\n",
    "            best_acc = 0.0\n",
    "            for idx, gs in enumerate(grids):\n",
    "                print('\\nEstimator: %s' % grid_dict[idx])\n",
    "                # Fit grid search\n",
    "                gs.fit(X_train, y_train)\n",
    "                # Best params\n",
    "                print('Best params: %s' % gs.best_params_)\n",
    "                # Best training data accuracy\n",
    "                print('Best training score: %.3f' % gs.best_score_)\n",
    "                # Predict on test data with best params\n",
    "                y_pred = gs.predict(X_test)\n",
    "                # Test data accuracy of model with best params\n",
    "                print('Test set score score for best params: %.3f ' % accuracy_score(y_test, y_pred))\n",
    "                # Track best (highest test accuracy) model\n",
    "                if accuracy_score(y_test, y_pred) > best_acc:\n",
    "                    best_acc = accuracy_score(y_test, y_pred)\n",
    "                    best_clf = idx\n",
    "            print('\\nClassifier with best test set score: %s' % grid_dict[best_clf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-3-b406bd087bdf>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-b406bd087bdf>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_0_std_1_ver2.xlsx\u001b[0m\n\u001b[1;37m                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_0_std_1_ver2.xlsx\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.001, 'classifier__l1_ratio': 0.2, 'classifier__n_iter': 5, 'classifier__tol': 0.001}\n",
    "Best training score: 0.654\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.1, 'classifier__l1_ratio': 0.3, 'classifier__n_iter': 3, 'classifier__tol': 0.01}\n",
    "Best training score: 0.710\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "Estimator: xgb short\n",
    "Best params: {'classifier__gamma': 0.5, 'classifier__learning_rate': 0.25, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
    "Best training score: 0.654\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 10, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
    "Best training score: 0.705\n",
    "Test set score score for best params: 0.630 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-4-e762ae5ade66>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-e762ae5ade66>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_1_std_0_ver2.xlsx\u001b[0m\n\u001b[1;37m                                                                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\training_data_pupil_size_1_std_0_ver2.xlsx\n",
    "Estimator: lr dual\n",
    "Best params: {'classifier__C': 0.01, 'classifier__tol': 1e-05}\n",
    "Best training score: 0.644\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: rf\n",
    "Best params: {'classifier__max_depth': 2, 'classifier__max_features': 'auto', 'classifier__min_samples_split': 0.5, 'classifier__n_estimators': 250}\n",
    "Best training score: 0.644\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.001, 'classifier__l1_ratio': 0.3, 'classifier__n_iter': 4, 'classifier__tol': 0.001}\n",
    "Best training score: 0.720\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 0.01, 'classifier__penalty': 'l2', 'classifier__solver': 'saga', 'classifier__tol': 0.001}\n",
    "Best training score: 0.728\n",
    "Test set score score for best params: 0.630 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-5-e6e94331b544>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-e6e94331b544>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_0.xlsx\u001b[0m\n\u001b[1;37m                                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_0.xlsx\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.0001, 'classifier__l1_ratio': 0.025, 'classifier__n_iter': 7, 'classifier__tol': 0.0001}\n",
    "Best training score: 0.663\n",
    "Test set score score for best params: 0.704 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-6-645d2ef72f1f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-645d2ef72f1f>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_1.xlsx\u001b[0m\n\u001b[1;37m                                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_0_std_1.xlsx\n",
    "\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.0001, 'classifier__l1_ratio': 0.2, 'classifier__n_iter': 3, 'classifier__tol': 0.0001}\n",
    "Best training score: 0.740\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.01, 'classifier__l1_ratio': 0, 'classifier__n_iter': 3, 'classifier__tol': 0.001}\n",
    "Best training score: 0.761\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.0001, 'classifier__l1_ratio': 0.7, 'classifier__n_iter': 7, 'classifier__tol': 1e-06}\n",
    "Best training score: 0.740\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 1000, 'classifier__gamma': 0.001, 'classifier__kernel': 'sigmoid'}\n",
    "Best training score: 0.702\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "Estimator: xgb short\n",
    "Best params: {'classifier__gamma': 0, 'classifier__learning_rate': 0.25, 'classifier__max_depth': 2, 'classifier__n_estimators': 300}\n",
    "Best training score: 0.678\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "\n",
    "Estimator: xgb short\n",
    "Best params: {'classifier__gamma': 0, 'classifier__learning_rate': 0.25, 'classifier__max_depth': 5, 'classifier__n_estimators': 300}\n",
    "Best training score: 0.655\n",
    "Test set score score for best params: 0.667 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-7-2b62deb29ab6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-7-2b62deb29ab6>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_0.xlsx\u001b[0m\n\u001b[1;37m                                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_0.xlsx\n",
    "\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.1, 'classifier__l1_ratio': 0.6, 'classifier__n_iter': 10, 'classifier__tol': 1e-06}\n",
    "Best training score: 0.740\n",
    "Test set score score for best params: 0.593 \n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 1, 'classifier__gamma': 0.1, 'classifier__kernel': 'rbf'}\n",
    "Best training score: 0.708\n",
    "Test set score score for best params: 0.630 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-8-a328f951f655>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-a328f951f655>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_1.xlsx\u001b[0m\n\u001b[1;37m                                                                                                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_1.xlsx\n",
    "\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.01, 'classifier__l1_ratio': 0.025,\n",
    "'classifier__n_iter': 7, 'classifier__tol': 0.01}\n",
    "Best training score: 0.813\n",
    "Test set score score for best params: 0.704 \n",
    "\n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 1000, 'classifier__penalty': 'l1',\n",
    "'classifier__solver': 'liblinear', 'classifier__tol': 0.0001}\n",
    "Best training score: 0.762\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: lr dual\n",
    "Best params: {'classifier__C': 100, 'classifier__tol': 0.0001}\n",
    "Best training score: 0.741\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 1, 'classifier__gamma': 1, 'classifier__kernel': 'linear'}\n",
    "Best training score: 0.768\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 10000, 'classifier__penalty': 'l1',\n",
    "'classifier__solver': 'warn', 'classifier__tol': 1e-05}\n",
    "Best training score: 0.756\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "Best params: {'classifier__alpha': 0.01, 'classifier__l1_ratio': 0.3, 'classifier__n_iter': 6, 'classifier__tol': 0.001}\n",
    "Best training score: 0.731\n",
    "Test set score score for best params: 0.704 \n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 10, 'classifier__penalty': 'l2', \n",
    "'classifier__solver': 'liblinear', 'classifier__tol': 1e-05}\n",
    "Best training score: 0.712\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Estimator: lr dual\n",
    "Best params: {'classifier__C': 10, 'classifier__tol': 1e-05}\n",
    "Best training score: 0.712\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 1000, 'classifier__gamma': 0.0001, 'classifier__kernel': 'sigmoid'}\n",
    "Best training score: 0.712\n",
    "Test set score score for best params: 0.704 \n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 10, 'classifier__penalty': 'l2',\n",
    "'classifier__solver': 'liblinear', 'classifier__tol': 1e-05}\n",
    "Best training score: 0.712\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.01, 'classifier__l1_ratio': 0.1, 'classifier__n_iter': 7, 'classifier__tol': 0.01}\n",
    "Best training score: 0.781\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 100, 'classifier__penalty': 'l1',\n",
    "'classifier__solver': 'warn', 'classifier__tol': 0.1}\n",
    "Best training score: 0.749\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "Estimator: lr dual\n",
    "Best params: {'classifier__C': 10, 'classifier__tol': 1e-05}\n",
    "Best training score: 0.739\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "\n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 1000, 'classifier__gamma': 0.0001, 'classifier__kernel': 'sigmoid'}\n",
    "Best training score: 0.739\n",
    "Test set score score for best params: 0.704 \n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 10000, 'classifier__penalty': 'l1',\n",
    "'classifier__solver': 'warn', 'classifier__tol': 0.1}\n",
    "Best training score: 0.773\n",
    "Test set score score for best params: 0.667 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C:\\‏‏PycharmProjects\\SocialAnxietyClassifier\\data\\training_data\\mean_analysis_trainingpupil_size_1_std_1.xlsx\n",
    "\n",
    "Estimator: en\n",
    "Best params: {'classifier__alpha': 0.0001, 'classifier__l1_ratio': 0.1,\n",
    "'classifier__n_iter': 5, 'classifier__tol': 1e-05}\n",
    "Best training score: 0.769\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "\n",
    "Estimator: lr\n",
    "Best params: {'classifier__C': 10, 'classifier__penalty': 'l1',\n",
    "'classifier__solver': 'saga', 'classifier__tol': 1e-05}\n",
    "Best training score: 0.731\n",
    "Test set score score for best params: 0.667 \n",
    "\n",
    "Estimator: lr dual\n",
    "Best params: {'classifier__C': 1, 'classifier__tol': 1e-05}\n",
    "Best training score: 0.721\n",
    "Test set score score for best params: 0.630 \n",
    "\n",
    "\n",
    "Estimator: svc\n",
    "Best params: {'classifier__C': 1, 'classifier__gamma': 1, \n",
    "'classifier__kernel': 'linear'}\n",
    "Best training score: 0.740\n",
    "Test set score score for best params: 0.630 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\externals\\six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorflow-2.0.0a0-py3.6-win-amd64.egg\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, roc_auc_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fancyimpute import IterativeImputer\n",
    "\n",
    "\n",
    "\n",
    "# seed\n",
    "# import os\n",
    "# import random\n",
    "# os.environ['PYTHONHASHSEED']=str(271828)\n",
    "# random.seed(271828)\n",
    "# np.random.seed(271828)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PCL_calculator(df):\n",
    "\n",
    "    symptomatic_cutoff = 2\n",
    "    intrusion = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH', 'q6.4_UPSET', 'q6.5_PHYS']\n",
    "    avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES', 'q6.9_DISINT', 'q6.10_DTACH',\n",
    "                 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "    tred = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH']\n",
    "    only_avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES']\n",
    "    hypertension = ['q6.13_SLEEP', 'q6.14_ANGER', 'q6.15_CONC', 'q6.16_HYPER', 'q6.17_STRTL']\n",
    "    depression = ['q6.9_DISINT', 'q6.10_DTACH', 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "\n",
    "    df[intrusion + avoidance + hypertension].fillna(df[intrusion + avoidance + hypertension].mean(axis=1))\n",
    "    intrusion_cuoff = 1\n",
    "    avoidance_cuoff = 3\n",
    "    hypertension_cuoff = 2\n",
    "    only_avoidance_cutoff = 1\n",
    "    depression_cutoff = 2\n",
    "    tred_cutoff = 1\n",
    "\n",
    "    df['PCL_score'] = (df[intrusion + avoidance + hypertension]).sum(axis=1)\n",
    "    df['PCL_mean'] = (df[intrusion + avoidance + hypertension]).mean(axis=1)\n",
    "    df['PCL_std'] = (df[intrusion + avoidance + hypertension]).std(axis=1)\n",
    "    \n",
    "\n",
    "    df['intrusion'] = (df[intrusion] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['intrusion_mean'] = (df[intrusion] > symptomatic_cutoff).mean(axis=1)\n",
    "    df['intrusion_std'] = (df[intrusion] > symptomatic_cutoff).std(axis=1)\n",
    "    df['intrusion_cutoff'] = (df['intrusion'] >= intrusion_cuoff).astype(int)\n",
    "\n",
    "    df['avoidance'] = (df[avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['avoidance_mean'] = (df[avoidance] > symptomatic_cutoff).mean(axis=1)\n",
    "    df['avoidance_std'] = (df[avoidance] > symptomatic_cutoff).std(axis=1)\n",
    "    df['avoidance_cutoff'] = (df['avoidance'] >= avoidance_cuoff).astype(int)\n",
    "\n",
    "    df['depression'] = (df[depression] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['depression_mean'] = (df[depression] > symptomatic_cutoff).mean(axis=1)\n",
    "    df['depression_std'] = (df[depression] > symptomatic_cutoff).std(axis=1)\n",
    "    df['depression_cutoff'] = (df['depression'] >= depression_cutoff).astype(int)\n",
    "\n",
    "    df['hypertention'] = (df[hypertension] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['hypertention_mean'] = (df[hypertension] > symptomatic_cutoff).mean(axis=1)\n",
    "    df['hypertention_std'] = (df[hypertension] > symptomatic_cutoff).std(axis=1)\n",
    "    df['hypertention_cutoff'] = (df['hypertention'] >= hypertension_cuoff).astype(int)\n",
    "\n",
    "    df['tred'] = (df[tred] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['tred_mean'] = (df[tred] > symptomatic_cutoff).mean(axis=1)\n",
    "    df['tred_std'] = (df[tred] > symptomatic_cutoff).std(axis=1)\n",
    "    df['tred_cutoff'] = df['tred'] >= tred_cutoff\n",
    "\n",
    "    df['only_avoidance'] = (df[only_avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['only_avoidance_mean'] = (df[only_avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['only_avoidance_std'] = (df[only_avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['only_avoidance_cutoff'] = df['only_avoidance'] >= only_avoidance_cutoff\n",
    "\n",
    "    #df['regression_cutoff_33'] = df['sum'] >= 33\n",
    "    #df['regression_cutoff_50'] = df['sum'] >= 49\n",
    "    df['diagnosis'] = ((df['hypertention_cutoff']) & (df['avoidance_cutoff']) & (df['intrusion_cutoff']) & (df['PCL_score'] >= 49))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\‏‏PycharmProjects\\PTSD\\Data\\PTSD.xlsx\"\n",
    "df = pd.read_excel(path)\n",
    "#combine with specifics of answers\n",
    "df_pcl2 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL2.xlsx\")\n",
    "df_pcl2 = PCL_calculator(df_pcl2)\n",
    "\n",
    "df_pcl1 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL1.xlsx\")\n",
    "df_pcl1 = PCL_calculator(df_pcl1)\n",
    "\n",
    "df = df.merge(df_pcl1, on=\"ID\", how='outer')\n",
    "df = df.merge(df_pcl2, suffixes=('_pcl1', '_pcl2'), on=\"ID\", how='outer')\n",
    "\n",
    "df_pcl3 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL3.xlsx\")\n",
    "df_pcl3 = PCL_calculator(df_pcl3)\n",
    "df = df.merge(df_pcl3.drop(['PCL3_Strict', 'pcl3', 'PCL3_Broad'], axis=1), on=\"ID\", how='outer')\n",
    "# rmoving missing Y's\n",
    "df = df[~df['PCL_Strict3'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"highschool_diploma\", \"dyslexia\", \"ADHD\", \"T1Acc1t\", \"T1Acc1n\", \"T1bias\", \"phq1\", \"lot1\",\n",
    "                \"trait1\",\n",
    "                \"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\", \"phq2\", \"lot2\", \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\",\n",
    "                \"PCL_Strict2\", \"cd_risc1\", \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\",\n",
    "                \"humor1\",\n",
    "                \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\", \"active_coping2\", \"planning2\",\n",
    "                \"positive_reframing2\", \"acceptance2\", \"humor2\", \"religion2\", \"emotional_support2\",\n",
    "                \"instrumental_support2\",\n",
    "                \"self_distraction2\", \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\",\n",
    "                \"self_blame2\",\n",
    "                \"trauma_history8_1\", \"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\", \"COMT_Hap1_recode\",\n",
    "                \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\", 'q6.1_INTRU_pcl1', \n",
    "                'q6.2_DREAM_pcl1', 'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                  'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                  'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                  'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1',\n",
    "                  'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2', 'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2',\n",
    "                  'q6.5_PHYS_pcl2', 'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                  'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                  'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2',\n",
    "            \n",
    "            'PCL_score_pcl1', 'PCL_mean_pcl1', 'PCL_std_pcl1', 'intrusion_pcl1', 'intrusion_mean_pcl1', 'intrusion_std_pcl1',\n",
    "            'intrusion_cutoff_pcl1', 'avoidance_pcl1', 'avoidance_mean_pcl1', 'avoidance_std_pcl1', 'avoidance_cutoff_pcl1', \n",
    "            'depression_pcl1', 'depression_mean_pcl1', 'depression_std_pcl1', 'depression_cutoff_pcl1', 'hypertention_pcl1',\n",
    "            'hypertention_mean_pcl1', 'hypertention_std_pcl1', 'hypertention_cutoff_pcl1', 'tred_pcl1', 'tred_mean_pcl1', \n",
    "            'tred_std_pcl1', 'tred_cutoff_pcl1', 'only_avoidance_pcl1','only_avoidance_mean_pcl1', 'only_avoidance_std_pcl1', \n",
    "            'only_avoidance_cutoff_pcl1',\n",
    "            \n",
    "            'PCL_score_pcl2', 'PCL_mean_pcl2', 'PCL_std_pcl2', 'intrusion_pcl2', 'intrusion_mean_pcl2', 'intrusion_std_pcl2',\n",
    "            'intrusion_cutoff_pcl2', 'avoidance_pcl2', 'avoidance_mean_pcl2', 'avoidance_std_pcl2', 'avoidance_cutoff_pcl2', \n",
    "            'depression_pcl2', 'depression_mean_pcl2', 'depression_std_pcl2', 'depression_cutoff_pcl2', 'hypertention_pcl2',\n",
    "            'hypertention_mean_pcl2', 'hypertention_std_pcl2', 'hypertention_cutoff_pcl2', 'tred_pcl2', 'tred_mean_pcl2', \n",
    "            'tred_std_pcl2', 'tred_cutoff_pcl2', 'only_avoidance_pcl2','only_avoidance_mean_pcl2', 'only_avoidance_std_pcl2', \n",
    "            'only_avoidance_cutoff_pcl2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features = [\"age\", \"highschool_diploma\", \"dyslexia\", \"ADHD\", \"phq1\", \"lot1\",\n",
    "                    \"trait1\", \"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\", \"phq2\", \"lot2\",\n",
    "                    \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\", \"PCL_Strict2\", \"cd_risc1\", \"active_coping1\",\n",
    "                    \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\", \"religion1\",\n",
    "                    \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                    \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\", \"active_coping2\",\n",
    "                    \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\", \"religion2\", \"emotional_support2\",\n",
    "                    \"instrumental_support2\", \"self_distraction2\", \"denial2\", \"venting2\", \"substance_use2\",\n",
    "                    \"behavioral_disengagement2\", \"self_blame2\", \"trauma_history8_1\",\n",
    "                    'q6.1_INTRU_pcl1', 'q6.2_DREAM_pcl1', 'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                    'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                    'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                    'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1',\n",
    "                    'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2', 'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2',\n",
    "                    'q6.5_PHYS_pcl2', 'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                    'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                    'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2',\n",
    "                    'intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_features = [\"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\", \"COMT_Hap1_recode\",\n",
    "                \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\"]\n",
    "continuous_features = [\"T1Acc1t\", \"T1Acc1n\", \"T1bias\"]\n",
    "t2_features = [\n",
    "    \"lot2\", \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\", \"PCL_Strict2\", \"phq2\",\n",
    "\n",
    "    \"active_coping2\", \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\", \n",
    "    \"religion2\", \"emotional_support2\", \"instrumental_support2\", \"self_distraction2\",\n",
    "    \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\", \"self_blame2\",\n",
    "    \n",
    "    'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2', 'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2', 'q6.5_PHYS_pcl2',\n",
    "    'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                    'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                    'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2'\n",
    "]\n",
    "t1_features =[ \"phq1\", \"lot1\", \"trait1\",\"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\",\n",
    "              \n",
    "                    \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\", \"religion1\",\n",
    "                    \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                    \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\",\n",
    "                'q6.1_INTRU_pcl1', 'q6.2_DREAM_pcl1', 'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                    'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                    'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                    'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_targets = ['intrusion_cutoff','hypertention_cutoff', 'avoidance_cutoff', 'depression_cutoff']\n",
    "target_feature = ['PCL_Strict3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[features + mtl_targets + target_feature]\n",
    "\n",
    "#df1 = df1.dropna(thresh=47)\n",
    "df_t1 = df1[t1_features]\n",
    "df_t2 = df1[t2_features]\n",
    "df1 = df1[(df_t1.isna().astype(int).sum(axis=1)<38) | (df_t2.isna().astype(int).sum(axis=1)<38)]\n",
    "df1[\"t1_missing\"] = df_t1.isna().astype(int).sum(axis=1)==38\n",
    "df1[\"t2_missing\"] = df_t2.isna().astype(int).sum(axis=1)==38\n",
    "features.extend([\"t1_missing\", \"t2_missing\"])\n",
    "\n",
    "impute = 1\n",
    "if impute:\n",
    "    for i in mtl_targets:\n",
    "        df1[i] = df[i].fillna(0)\n",
    "    mice = IterativeImputer()\n",
    "    df1 = pd.DataFrame(mice.fit_transform(df1), columns=df1.columns)\n",
    "    #Y = Y.fillna(Y.mean())\n",
    "    features.extend([\"outliers_count_t1_35\", \"outliers_count_t2_35\", \"outliers_count_t1_25\", \"outliers_count_t2_25\"])\n",
    "    df1[\"outliers_count_t1_35\"] = (np.abs(stats.zscore(df1[t1_features])) > 3.5).sum(axis=1)\n",
    "    df1[\"outliers_count_t1_25\"] = (np.abs(stats.zscore(df1[t1_features])) > 2.5).sum(axis=1)\n",
    "    df1[\"outliers_count_t2_35\"] = (np.abs(stats.zscore(df1[t2_features])) > 3.5).sum(axis=1)\n",
    "    df1[\"outliers_count_t2_25\"] = (np.abs(stats.zscore(df1[t2_features])) > 2.5).sum(axis=1)\n",
    "    \n",
    "    \n",
    "#df1 = df1[(np.abs(stats.zscore(df1)) > 3).sum(axis=1)<15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1[features] =StandardScaler().fit_transform(df1[features]), columns=df1.columns)\n",
    "df1['std_genome'] = df1[genome_features].std(axis=1)\n",
    "df1['std_t2'] = df1[[ \"trait2\", \"state2\", \"PCL2\"]].std(axis=1)\n",
    "df1['std_t1'] = df1[[ \"trait1\", \"state1\", \"PCL1\"]].std(axis=1)\n",
    "\n",
    "df1['std_pos_coping_t1'] = df1[[\"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\",\n",
    "                \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\",\"venting1\"]].std(axis=1)\n",
    "\n",
    "df1['std_neg_coping_t1'] = df1[[\"denial1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\"]].std(axis=1)\n",
    "\n",
    "df1['std_pos_coping_t2'] = df1[[\"active_coping2\", \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\",\n",
    "                \"religion1\", \"emotional_support2\", \"instrumental_support2\", \"self_distraction2\",\"venting2\"]].std(axis=1)\n",
    "\n",
    "df1['std_neg_coping_t2'] = df1[[\"denial2\", \"substance_use2\", \"behavioral_disengagement2\", \"self_blame2\"]].std(axis=1)\n",
    "\n",
    "features = features + ['std_genome', 'std_t1', 'std_t2', 'std_pos_coping_t1', 'std_neg_coping_t1', 'std_pos_coping_t2', 'std_neg_coping_t2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut off the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_out, Y, y_out = train_test_split(df1[features + mtl_targets], df1[target_feature[0]],\n",
    "                                      test_size=0.25, random_state=271828, stratify=df1[target_feature[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, stratify=Y, random_state=271828)\n",
    "ss, tt = StandardScaler(), StandardScaler()\n",
    "X_train, X_test = pd.DataFrame(np.hstack([ss.fit_transform(X_train[features]), X_train[mtl_targets]]), columns=X_train.columns), pd.DataFrame(np.hstack([tt.fit_transform(X_test[features]), X_test[mtl_targets]]), columns=X_test.columns)\n",
    "cv = StratifiedKFold(5, random_state=271828)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV model of roc auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "scores_f = []\n",
    "scores_p = []\n",
    "scores_r = []\n",
    "scores_auc = []\n",
    "\n",
    "for train, test in cv.split(X_train, y_train.astype(int)):\n",
    "\n",
    "    X_train_cv = X_train.iloc[train]\n",
    "    y_train_cv = y_train.iloc[train]\n",
    "\n",
    "    t = [X_train_cv[i] for i in mtl_targets] + [y_train_cv]\n",
    "    X_train_cv.drop(mtl_targets, axis=1, inplace=True)\n",
    "    y_pred_1 = CatBoostClassifier(verbose=0, loss_function='Logloss',class_weights=[1, 3], n_estimators=500).fit(X_train_cv, (t[0]).values.astype(int)).predict_proba(X_train.iloc[test])\n",
    "    y_pred_2 = CatBoostClassifier(verbose=0, loss_function='Logloss',class_weights=[1, 3], n_estimators=500).fit(X_train_cv, (t[1]).values.astype(int)).predict_proba(X_train.iloc[test])\n",
    "    y_pred_3 = CatBoostClassifier(verbose=0, loss_function='Logloss',class_weights=[1, 3], n_estimators=500).fit(X_train_cv, (t[2]).values.astype(int)).predict_proba(X_train.iloc[test])\n",
    "    y_pred_target = CatBoostClassifier(verbose=0, loss_function='Logloss',class_weights=[1, 20], n_estimators=500).fit(X_train_cv, t[3].values.astype(int)).predict_proba(X_train.iloc[test])\n",
    "\n",
    "    y_pred = (y_pred_1[:, 1] * y_pred_2[:, 1] *  y_pred_3[:, 1] * y_pred_target[:, 1]) > 0.02625 #+ 0.5 * y_pred_target[:, 1]) > 1\n",
    "\n",
    "    scores_f.append(f1_score(y_train.iloc[test], y_pred))\n",
    "    scores_p.append(precision_score(y_train.iloc[test], y_pred))\n",
    "    scores_r.append(recall_score(y_train.iloc[test], y_pred))\n",
    "    scores_auc.append(roc_auc_score(y_train.iloc[test], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.8421052631578948\n",
      "p 0.7272727272727273\n",
      "r 1.0\n",
      "auc 0.9879032258064516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = [X_train[i] for i in mtl_targets] + [y_train]\n",
    "X_train.drop(mtl_targets, axis=1, inplace=True)\n",
    "    \n",
    "y_pred_1 = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X_train, (t[0]).values.astype(int)).predict_proba(X_test)\n",
    "y_pred_2 = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X_train, (t[1]).values.astype(int)).predict_proba(X_test)\n",
    "y_pred_3 = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X_train, (t[2]).values.astype(int)).predict_proba(X_test)\n",
    "y_pred_target = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X_train, t[3].values.astype(int)).predict_proba(X_test)\n",
    "\n",
    "y_pred = (y_pred_1[:, 1] * y_pred_2[:, 1] *  y_pred_3[:, 1] * y_pred_target[:, 1]) > 0.0625 #+ 0.5 * y_pred_target[:, 1]) > 1\n",
    "\n",
    "print(\"f1\", f1_score(y_test.astype(bool), y_pred))\n",
    "print(\"p\",precision_score(y_test.astype(bool), y_pred))\n",
    "print(\"r\",recall_score(y_test.astype(bool), y_pred))\n",
    "print(\"auc\", roc_auc_score(y_test.astype(bool), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     highschool_diploma  dyslexia      ADHD   T1Acc1t   T1Acc1n     T1bias  \\\n",
       " 174            1.000000  0.000000  0.000000  1.000000  1.000000   7.360000   \n",
       " 459            1.000000  0.000000  0.000000  0.986800  0.986800   0.820000   \n",
       " 635            1.000000  0.000000  0.000000  0.983790  0.967737   0.489731   \n",
       " 78             1.000000  0.000000  0.000000  0.973700  0.960500  21.500000   \n",
       " 548            1.000000  0.000000  0.000000  0.986800  1.000000   1.620000   \n",
       " 358            1.000000  0.000000  0.000000  0.986800  0.973700 -13.380000   \n",
       " 339            1.000000  0.000000  0.000000  1.000000  0.986800 -15.590000   \n",
       " 490            0.000000  0.000000  0.000000  0.986800  0.973700 -38.460000   \n",
       " 421            1.000000  0.000000  0.000000  1.000000  1.000000  -0.620000   \n",
       " 282            1.000000  1.000000  1.000000  0.973700  1.000000  -6.230000   \n",
       " 89             1.000000  0.000000  0.000000  1.000000  1.000000  11.330000   \n",
       " 357            1.000000  0.000000  0.000000  1.000000  1.000000   6.110000   \n",
       " 62             0.000000  0.000000  0.000000  1.000000  0.986800  29.050000   \n",
       " 251            1.000000  0.000000  1.000000  0.986800  1.000000 -18.500000   \n",
       " 495            1.000000  0.000000  0.000000  0.973700  0.973700   8.670000   \n",
       " 185            1.000000  0.000000  0.000000  0.977192  0.968324  -4.829241   \n",
       " 151            1.000000  0.000000  0.000000  0.960500  0.973700 -18.460000   \n",
       " 44             1.000000  0.000000  0.000000  0.986800  1.000000  23.850000   \n",
       " 614            1.000000  0.000000  0.000000  1.003496  0.981139   2.517652   \n",
       " 394            1.000000  1.000000  1.000000  1.000000  0.986800   1.840000   \n",
       " 406            0.000000  0.000000  0.000000  0.973700  1.000000  19.340000   \n",
       " 112            1.000000  0.000000  0.000000  0.960500  0.960500 -26.000000   \n",
       " 149            1.000000  0.000000  0.000000  0.967408  0.940349   7.114591   \n",
       " 235            0.000000  1.000000  1.000000  0.960500  0.868400 -16.220000   \n",
       " 242            1.000000  0.000000  0.000000  0.921100  0.960500   3.480000   \n",
       " 694            0.000000  0.000000  0.000000  0.988874  0.969150  12.770709   \n",
       " 307            0.000000  0.000000  0.000000  0.986800  1.000000   2.740000   \n",
       " 554            1.000000  1.000000  1.000000  0.986800  1.000000   1.620000   \n",
       " 341            0.000000  1.000000  1.000000  1.000000  0.986800 -16.200000   \n",
       " 265            1.000000  0.000000  0.000000  1.000000  1.000000   7.920000   \n",
       " ..                  ...       ...       ...       ...       ...        ...   \n",
       " 175            1.000000  0.000000  1.000000  1.000000  1.000000 -10.350000   \n",
       " 74             0.000000  1.000000  1.000000  1.000000  0.986800 -12.890000   \n",
       " 237            1.000000  0.000000  0.000000  1.000000  0.973700 -14.910000   \n",
       " 211            1.000000  0.000000  0.000000  0.973700  0.947400  -3.390000   \n",
       " 695            1.000000  0.000000  0.000000  0.992389  0.961738   5.138564   \n",
       " 234            1.000000  0.000000  0.000000  0.967613  0.982478 -16.075519   \n",
       " 120            0.000000  0.000000  0.000000  0.986800  0.986800   6.080000   \n",
       " 493            1.000000  0.000000  0.000000  1.000000  0.986800  -0.330000   \n",
       " 432            1.000000  0.000000  0.000000  1.000000  1.000000  19.590000   \n",
       " 688            0.000000  0.000000  0.000000  1.003266  0.964418  16.703681   \n",
       " 453            0.000000  0.110393  0.088222  1.000000  1.000000   8.850000   \n",
       " 189            0.000000  0.000000  0.000000  0.934200  0.921100  42.680000   \n",
       " 328            0.853759  0.000000  0.000000  0.986800  0.960500  -2.340000   \n",
       " 381            1.000000  0.000000  0.000000  0.986800  0.986800  -9.390000   \n",
       " 97             1.000000  0.000000  0.000000  0.986800  0.986800  17.150000   \n",
       " 569            1.000000  0.000000  0.000000  1.000000  0.986800  -9.990000   \n",
       " 678            0.000000  0.000000  0.000000  0.979740  0.967355   3.853077   \n",
       " 70             1.000000  0.000000  0.000000  1.000000  0.986800   6.500000   \n",
       " 172            1.000000  0.000000  1.000000  0.960500  0.986800 -19.390000   \n",
       " 87             0.000000  1.000000  1.000000  0.960500  0.986800  -0.860000   \n",
       " 116            1.000000  0.000000  0.000000  0.986800  0.986800   3.780000   \n",
       " 291            1.000000  0.000000  0.000000  1.000000  0.973700  25.510000   \n",
       " 12             1.000000  0.000000  0.000000  0.973700  1.000000 -18.220000   \n",
       " 63             1.000000  1.000000  1.000000  1.000000  0.986800  -3.510000   \n",
       " 371            1.000000  0.000000  0.000000  0.986800  1.000000 -10.270000   \n",
       " 578            1.000000  0.000000  1.000000  0.995603  0.973376  -8.861208   \n",
       " 576            1.000000  0.000000  0.000000  0.985477  0.965083  -2.217672   \n",
       " 455            1.000000  0.000000  0.000000  1.000000  1.000000  -3.660000   \n",
       " 543            0.000000  0.000000  1.000000  1.000000  1.000000   8.690000   \n",
       " 137            1.000000  0.000000  0.000000  0.986800  1.000000  -4.670000   \n",
       " \n",
       "           phq1       lot1     trait1     state1  ...     std_t1     std_t2  \\\n",
       " 174   4.000000  14.000000  27.000000  30.000000  ...   6.806859   6.429101   \n",
       " 459   4.000000  12.000000  25.000000  22.000000  ...   4.041452   2.081666   \n",
       " 635   5.801488  17.342425  37.281249  34.309002  ...   6.217215   9.523902   \n",
       " 78    4.000000  14.000000  37.000000  38.000000  ...   8.784192  12.165525   \n",
       " 548   2.000000  18.000000  24.000000  23.000000  ...   1.000000   4.372901   \n",
       " 358   7.000000  16.000000  35.000000  28.000000  ...   5.372572   3.464102   \n",
       " 339   7.000000  16.000000  28.000000  23.000000  ...   2.516611   7.371115   \n",
       " 490   3.000000  15.600000  32.000000  28.000000  ...   7.767453   4.369096   \n",
       " 421   0.000000   6.000000  29.000000  30.000000  ...   6.658328   1.732051   \n",
       " 282   8.000000  14.000000  49.000000  50.000000  ...  15.307950   9.865766   \n",
       " 89    2.000000  14.000000  39.000000  42.110000  ...  12.541559   9.018500   \n",
       " 357   1.000000  10.000000  34.000000  25.000000  ...   8.020806  11.015141   \n",
       " 62    9.000000  13.000000  51.000000  32.000000  ...  10.016653   9.848858   \n",
       " 251  23.000000  16.000000  59.000000  53.000000  ...  13.613719  13.279056   \n",
       " 495   4.000000  18.000000  32.000000  26.000000  ...   4.163332   5.686241   \n",
       " 185   2.000000  14.000000  25.260000  25.000000  ...   4.695658   4.725816   \n",
       " 151  15.000000  14.000000  60.000000  61.000000  ...  13.576941  17.776389   \n",
       " 44    6.000000  15.000000  25.000000  26.000000  ...   4.358899   3.055050   \n",
       " 614   8.208303  16.684027  35.575034  37.553712  ...   6.556148   9.504385   \n",
       " 394   9.000000  16.000000  40.000000  41.000000  ...   8.386497  11.846237   \n",
       " 406  10.000000  18.000000  37.000000  26.000000  ...   6.082763   3.000000   \n",
       " 112   0.000000  15.000000  27.000000  32.000000  ...   6.027714   3.214550   \n",
       " 149  10.000000  20.000000  51.000000  52.000000  ...   8.386497   8.082904   \n",
       " 235   9.000000  17.000000  42.000000  44.000000  ...  10.440307  10.692677   \n",
       " 242   6.000000  14.000000  28.000000  26.000000  ...   5.859465   7.440809   \n",
       " 694   6.483208  14.002553  30.892708  26.014285  ...   3.030397   5.291503   \n",
       " 307   8.000000  13.000000  25.000000  28.000000  ...   2.516611   6.506407   \n",
       " 554   0.000000  18.000000  28.000000  21.000000  ...   5.567764  10.503968   \n",
       " 341   1.000000  18.000000  35.000000  31.000000  ...   7.211103  12.935989   \n",
       " 265   0.000000  15.000000  33.680000  36.000000  ...   8.646047  10.692677   \n",
       " ..         ...        ...        ...        ...  ...        ...        ...   \n",
       " 175   3.000000  14.000000  27.000000  33.000000  ...   6.000000   8.386497   \n",
       " 74    1.000000  17.000000  30.000000  39.000000  ...  11.060440   2.081666   \n",
       " 237   5.358628  15.112369  32.614351  32.027806  ...   4.041661   7.815730   \n",
       " 211   7.000000  12.000000  46.000000  55.000000  ...  17.058722   9.165151   \n",
       " 695   7.177108  15.407183  34.339838  33.102556  ...   4.878711   8.082904   \n",
       " 234   8.000000  13.000000  34.000000  44.000000  ...   5.063082  10.066446   \n",
       " 120   4.000000  15.000000  29.000000  30.530000  ...   1.274219   4.358899   \n",
       " 493   5.000000  14.000000  41.000000  39.000000  ...   8.717798  11.357817   \n",
       " 432   3.000000  18.000000  28.000000  40.000000  ...  11.015141   7.211103   \n",
       " 688   8.293543  16.652976  39.491171  44.329400  ...   9.868727  12.124356   \n",
       " 453   1.000000  13.000000  33.000000  32.630000  ...   6.246783   8.526666   \n",
       " 189   6.000000  14.000000  43.000000  46.000000  ...  11.357817  14.422205   \n",
       " 328   3.000000  16.000000  33.000000  36.000000  ...   5.131601   8.144528   \n",
       " 381   1.000000  18.000000  25.000000  20.000000  ...   2.645751   3.064642   \n",
       " 97    7.000000  16.000000  41.000000  44.000000  ...   1.732051   2.516611   \n",
       " 569   5.000000  19.000000  34.000000  22.000000  ...   8.736895   4.618802   \n",
       " 678   7.743182  14.670354  39.598431  36.907771  ...   7.576447  14.394916   \n",
       " 70   10.000000  18.000000  28.000000  33.000000  ...   4.041452   2.645751   \n",
       " 172   0.000000  17.000000  35.000000  31.000000  ...   6.110101   9.451631   \n",
       " 87   18.000000  17.000000  35.000000  60.000000  ...  12.897028   3.055050   \n",
       " 116  10.000000  19.000000  45.000000  32.000000  ...   6.658328   7.000000   \n",
       " 291   2.000000  17.000000  33.000000  26.000000  ...   7.000000  15.132746   \n",
       " 12    1.000000  13.000000  28.000000  28.000000  ...   5.773503  10.148892   \n",
       " 63    6.000000  13.000000  42.000000  42.000000  ...   6.928203   7.571878   \n",
       " 371   1.000000  21.000000  24.000000  20.000000  ...   3.511885   6.658328   \n",
       " 578   4.896253  16.807123  36.368554  40.376258  ...   7.787145  19.974984   \n",
       " 576   4.845282  14.676430  35.908498  37.128204  ...   6.483116  16.802778   \n",
       " 455   3.000000  23.000000  34.000000  28.000000  ...   6.506407   5.686241   \n",
       " 543   3.000000  17.000000  35.000000  24.000000  ...   8.621678  10.598742   \n",
       " 137   5.000000  14.000000  39.000000  40.000000  ...   7.234178   6.110101   \n",
       " \n",
       "      std_pos_coping_t1  std_neg_coping_t1  std_pos_coping_t2  \\\n",
       " 174           1.969207           1.414214           1.523884   \n",
       " 459           2.616189           0.000000           2.750757   \n",
       " 635           0.897036           1.672180           1.589275   \n",
       " 78            1.581139           1.500000           1.686548   \n",
       " 548           1.751190           1.154701           1.257483   \n",
       " 358           2.002776           0.500000           1.837873   \n",
       " 339           2.213594           2.500000           1.943651   \n",
       " 490           2.065591           1.000000           1.510884   \n",
       " 421           2.162817           0.577350           2.201010   \n",
       " 282           0.823273           3.201562           1.247219   \n",
       " 89            2.319004           2.828427           2.573368   \n",
       " 357           1.595131           1.414214           2.054805   \n",
       " 62            1.577621           1.707825           1.135292   \n",
       " 251           1.646545           3.000000           1.494434   \n",
       " 495           1.619328           1.892969           1.632993   \n",
       " 185           2.347576           0.000000           2.270585   \n",
       " 151           1.828782           2.217356           1.712698   \n",
       " 44            2.319004           0.500000           2.065591   \n",
       " 614           1.688276           1.401220           1.651774   \n",
       " 394           0.816497           1.000000           1.414214   \n",
       " 406           1.549193           1.707825           1.505545   \n",
       " 112           1.988858           0.000000           1.619328   \n",
       " 149           1.969207           1.000000           1.636392   \n",
       " 235           2.412928           0.000000           1.523884   \n",
       " 242           2.273030           0.000000           1.567021   \n",
       " 694           1.579682           0.927622           2.093381   \n",
       " 307           1.712698           1.632993           1.354006   \n",
       " 554           1.888562           1.500000           2.263233   \n",
       " 341           1.779513           1.500000           1.418136   \n",
       " 265           2.270585           1.000000           2.406011   \n",
       " ..                 ...                ...                ...   \n",
       " 175           1.897367           0.000000           1.286684   \n",
       " 74            2.270585           1.500000           2.540779   \n",
       " 237           1.133200           1.637357           1.656529   \n",
       " 211           1.900292           2.362908           1.505545   \n",
       " 695           1.611990           1.950516           2.472311   \n",
       " 234           1.490712           1.732051           2.750757   \n",
       " 120           1.888562           0.500000           1.825742   \n",
       " 493           1.911951           2.872281           2.149935   \n",
       " 432           2.668749           0.500000           1.712698   \n",
       " 688           0.892981           1.336968           1.430603   \n",
       " 453           1.712698           0.957427           1.475730   \n",
       " 189           2.057507           2.061553           1.988858   \n",
       " 328           1.109080           1.062484           1.443638   \n",
       " 381           2.002776           2.000000           1.103871   \n",
       " 97            0.707107           1.632993           1.247219   \n",
       " 569           1.852926           2.500000           2.406011   \n",
       " 678           1.226573           1.075991           0.887374   \n",
       " 70            2.110819           0.000000           1.490712   \n",
       " 172           1.663330           0.957427           2.162817   \n",
       " 87            2.121320           2.217356           1.649916   \n",
       " 116           2.469818           3.000000           2.394438   \n",
       " 291           1.766981           0.500000           1.567021   \n",
       " 12            1.888562           1.000000           1.414214   \n",
       " 63            1.354006           0.500000           1.333333   \n",
       " 371           2.250926           0.000000           1.888562   \n",
       " 578           0.751789           0.375070           0.409930   \n",
       " 576           1.741473           1.388571           1.756227   \n",
       " 455           1.897367           1.000000           1.505545   \n",
       " 543           1.505545           2.000000           2.002776   \n",
       " 137           0.948683           1.500000           0.816497   \n",
       " \n",
       "      std_neg_coping_t2  intrusion_cutoff  hypertention_cutoff  \\\n",
       " 174           1.290994               0.0                  0.0   \n",
       " 459           0.500000               0.0                  0.0   \n",
       " 635           1.707825               0.0                  0.0   \n",
       " 78            1.000000               0.0                  0.0   \n",
       " 548           0.879206               0.0                  0.0   \n",
       " 358           1.414214               0.0                  0.0   \n",
       " 339           1.707825               1.0                  1.0   \n",
       " 490           1.149004               0.0                  0.0   \n",
       " 421           0.000000               0.0                  0.0   \n",
       " 282           1.500000               0.0                  0.0   \n",
       " 89            3.000000               0.0                  1.0   \n",
       " 357           1.290994               0.0                  0.0   \n",
       " 62            1.414214               1.0                  0.0   \n",
       " 251           2.629956               1.0                  0.0   \n",
       " 495           1.707825               0.0                  0.0   \n",
       " 185           1.000000               0.0                  0.0   \n",
       " 151           1.707825               0.0                  1.0   \n",
       " 44            0.957427               0.0                  0.0   \n",
       " 614           0.577350               0.0                  0.0   \n",
       " 394           1.154701               0.0                  0.0   \n",
       " 406           2.217356               0.0                  0.0   \n",
       " 112           0.000000               0.0                  0.0   \n",
       " 149           1.732051               1.0                  1.0   \n",
       " 235           1.000000               0.0                  1.0   \n",
       " 242           1.267780               0.0                  0.0   \n",
       " 694           0.957427               1.0                  1.0   \n",
       " 307           1.500000               0.0                  0.0   \n",
       " 554           2.000000               0.0                  0.0   \n",
       " 341           2.000000               0.0                  0.0   \n",
       " 265           0.000000               0.0                  0.0   \n",
       " ..                 ...               ...                  ...   \n",
       " 175           0.577350               0.0                  1.0   \n",
       " 74            0.500000               0.0                  0.0   \n",
       " 237           2.449490               0.0                  0.0   \n",
       " 211           2.362908               1.0                  0.0   \n",
       " 695           1.892969               1.0                  0.0   \n",
       " 234           1.000000               1.0                  1.0   \n",
       " 120           0.957427               0.0                  0.0   \n",
       " 493           1.500000               1.0                  1.0   \n",
       " 432           1.500000               0.0                  0.0   \n",
       " 688           0.957427               1.0                  1.0   \n",
       " 453           2.061553               0.0                  0.0   \n",
       " 189           1.500000               0.0                  0.0   \n",
       " 328           1.000000               0.0                  0.0   \n",
       " 381           1.396340               0.0                  0.0   \n",
       " 97            0.957427               1.0                  0.0   \n",
       " 569           2.581989               0.0                  0.0   \n",
       " 678           1.157611               0.0                  1.0   \n",
       " 70            2.160247               1.0                  0.0   \n",
       " 172           2.000000               0.0                  0.0   \n",
       " 87            1.500000               0.0                  1.0   \n",
       " 116           2.828427               1.0                  0.0   \n",
       " 291           0.957427               0.0                  0.0   \n",
       " 12            0.577350               0.0                  0.0   \n",
       " 63            0.500000               0.0                  0.0   \n",
       " 371           1.000000               0.0                  0.0   \n",
       " 578           0.000000               0.0                  0.0   \n",
       " 576           1.914854               0.0                  1.0   \n",
       " 455           1.290994               0.0                  0.0   \n",
       " 543           1.000000               0.0                  0.0   \n",
       " 137           1.414214               0.0                  0.0   \n",
       " \n",
       "      avoidance_cutoff  depression_cutoff  \n",
       " 174               0.0                0.0  \n",
       " 459               0.0                0.0  \n",
       " 635               0.0                0.0  \n",
       " 78                0.0                0.0  \n",
       " 548               0.0                0.0  \n",
       " 358               0.0                0.0  \n",
       " 339               1.0                1.0  \n",
       " 490               0.0                0.0  \n",
       " 421               0.0                0.0  \n",
       " 282               0.0                0.0  \n",
       " 89                0.0                0.0  \n",
       " 357               0.0                0.0  \n",
       " 62                0.0                0.0  \n",
       " 251               0.0                0.0  \n",
       " 495               0.0                0.0  \n",
       " 185               0.0                0.0  \n",
       " 151               1.0                1.0  \n",
       " 44                0.0                0.0  \n",
       " 614               0.0                0.0  \n",
       " 394               0.0                0.0  \n",
       " 406               0.0                0.0  \n",
       " 112               0.0                0.0  \n",
       " 149               1.0                1.0  \n",
       " 235               0.0                0.0  \n",
       " 242               0.0                0.0  \n",
       " 694               1.0                1.0  \n",
       " 307               0.0                0.0  \n",
       " 554               0.0                0.0  \n",
       " 341               0.0                0.0  \n",
       " 265               0.0                0.0  \n",
       " ..                ...                ...  \n",
       " 175               0.0                0.0  \n",
       " 74                0.0                0.0  \n",
       " 237               0.0                0.0  \n",
       " 211               0.0                0.0  \n",
       " 695               0.0                0.0  \n",
       " 234               1.0                1.0  \n",
       " 120               0.0                0.0  \n",
       " 493               0.0                0.0  \n",
       " 432               0.0                0.0  \n",
       " 688               0.0                0.0  \n",
       " 453               1.0                1.0  \n",
       " 189               0.0                0.0  \n",
       " 328               0.0                0.0  \n",
       " 381               0.0                0.0  \n",
       " 97                0.0                0.0  \n",
       " 569               0.0                0.0  \n",
       " 678               1.0                1.0  \n",
       " 70                0.0                0.0  \n",
       " 172               0.0                0.0  \n",
       " 87                1.0                0.0  \n",
       " 116               0.0                1.0  \n",
       " 291               0.0                0.0  \n",
       " 12                0.0                0.0  \n",
       " 63                0.0                0.0  \n",
       " 371               0.0                0.0  \n",
       " 578               0.0                0.0  \n",
       " 576               0.0                0.0  \n",
       " 455               0.0                0.0  \n",
       " 543               0.0                0.0  \n",
       " 137               0.0                0.0  \n",
       " \n",
       " [528 rows x 162 columns],\n",
       "      highschool_diploma  dyslexia      ADHD   T1Acc1t   T1Acc1n      T1bias  \\\n",
       " 524                 1.0  0.000000  0.000000  1.000000  0.986800  -10.970000   \n",
       " 566                 1.0  0.000000  0.000000  0.973700  1.000000   19.050000   \n",
       " 347                 1.0  0.000000  0.000000  1.000000  1.000000    2.040000   \n",
       " 584                 1.0  0.000000  0.000000  0.994044  0.978368   -2.601300   \n",
       " 136                 1.0  0.000000  1.000000  0.977792  0.982996    7.022811   \n",
       " 581                 1.0  0.000000  0.000000  0.957143  0.965333    8.794972   \n",
       " 68                  1.0  0.000000  0.000000  1.000000  1.000000  -14.300000   \n",
       " 107                 1.0  0.000000  0.000000  0.894700  0.921100   10.150000   \n",
       " 615                 0.0  0.000000  0.000000  0.975138  0.966461   -9.332340   \n",
       " 272                 1.0  0.000000  0.000000  0.960500  0.973700  -19.970000   \n",
       " 204                 1.0  0.000000  0.000000  0.960500  1.000000   10.920000   \n",
       " 690                 0.0  0.000000  0.000000  0.997421  0.968960   -4.636361   \n",
       " 550                 0.0  0.000000  0.000000  0.986800  0.960500   24.230000   \n",
       " 583                 1.0  0.000000  0.000000  0.973793  0.964216   -2.670577   \n",
       " 613                 1.0  0.000000  0.000000  0.981317  0.981304   -1.847089   \n",
       " 456                 1.0  0.000000  0.000000  0.986800  0.960500    9.610000   \n",
       " 66                  1.0  0.000000  1.000000  1.000000  1.000000   11.340000   \n",
       " 144                 1.0  0.000000  0.000000  0.972136  0.970111    5.489792   \n",
       " 201                 1.0  1.000000  1.000000  1.000000  1.000000    6.100000   \n",
       " 645                 0.0  0.000000  0.000000  0.991448  0.968676   -5.555893   \n",
       " 521                 1.0  0.000000  0.000000  1.000000  1.000000   -9.210000   \n",
       " 255                 1.0  0.000000  0.000000  0.986800  1.000000   -8.020000   \n",
       " 506                 1.0  0.000000  0.000000  0.960500  0.986800   15.660000   \n",
       " 50                  1.0  0.000000  0.000000  1.000000  1.000000   13.040000   \n",
       " 111                 1.0  0.000000  0.000000  0.986800  0.973700   -3.130000   \n",
       " 88                  1.0  0.000000  0.000000  0.960500  0.986800  -20.220000   \n",
       " 402                 0.0  0.000000  1.000000  0.986800  0.973700   -5.760000   \n",
       " 125                 1.0  0.000000  0.000000  0.986800  0.934200   -6.350000   \n",
       " 529                 1.0  0.000000  0.000000  1.000000  0.986800   -7.940000   \n",
       " 249                 1.0  0.000000  0.000000  1.000000  0.973700  -15.380000   \n",
       " ..                  ...       ...       ...       ...       ...         ...   \n",
       " 246                 1.0  0.000000  0.000000  1.000000  0.960500  -15.390000   \n",
       " 30                  0.0  0.000000  0.000000  0.973700  0.973700   17.540000   \n",
       " 24                  1.0  0.000000  0.000000  0.986800  0.986800   -4.060000   \n",
       " 422                 0.0  0.000000  0.000000  0.983685  0.981743  -11.707613   \n",
       " 618                 1.0  0.109792  0.139019  0.977519  0.968957    2.069951   \n",
       " 385                 1.0  0.000000  0.000000  0.960500  0.973700  441.120000   \n",
       " 206                 1.0  1.000000  1.000000  1.000000  0.973700   40.770000   \n",
       " 375                 1.0  0.000000  0.000000  1.000000  1.000000    6.780000   \n",
       " 134                 1.0  0.000000  0.000000  0.921100  0.986800  -16.850000   \n",
       " 314                 0.0  0.000000  0.000000  0.934200  0.986800  -19.360000   \n",
       " 553                 1.0  0.000000  0.000000  1.000000  1.000000    8.970000   \n",
       " 525                 1.0  0.000000  0.000000  0.960500  0.960500   -0.910000   \n",
       " 162                 1.0  0.000000  0.000000  0.973700  0.960500   -7.520000   \n",
       " 45                  1.0  0.000000  0.000000  0.967721  0.982402  -22.645002   \n",
       " 366                 0.0  0.000000  1.000000  0.973700  0.973700   19.890000   \n",
       " 561                 1.0  1.000000  0.000000  1.000000  1.000000   12.510000   \n",
       " 301                 1.0  0.000000  0.000000  0.973700  0.986800   -1.430000   \n",
       " 530                 0.0  0.000000  0.000000  0.960500  1.000000   -0.070000   \n",
       " 109                 1.0  0.000000  0.000000  1.000000  0.973700   10.510000   \n",
       " 91                  1.0  0.000000  1.000000  0.986800  0.986800  -21.850000   \n",
       " 522                 1.0  0.000000  0.000000  0.960500  0.960500   -8.960000   \n",
       " 703                 0.0  0.000000  0.000000  0.976843  0.968031   -0.693790   \n",
       " 428                 1.0  0.000000  1.000000  0.986800  0.986800   -9.140000   \n",
       " 384                 0.0  0.000000  0.000000  0.986800  0.986800   -3.660000   \n",
       " 411                 1.0  0.000000  0.000000  1.000000  1.000000  -10.190000   \n",
       " 620                 1.0  0.000000  0.000000  0.979610  0.966379   -5.901881   \n",
       " 163                 0.0  0.000000  0.000000  0.947400  0.934200   -0.490000   \n",
       " 480                 1.0  0.000000  0.000000  1.000000  1.000000   -6.290000   \n",
       " 278                 1.0  0.000000  0.000000  1.000000  1.000000   23.000000   \n",
       " 503                 1.0  0.000000  0.000000  0.934200  0.934200   -4.090000   \n",
       " \n",
       "           phq1       lot1     trait1     state1  ...     std_t1     std_t2  \\\n",
       " 524   0.000000  14.000000  23.000000  21.000000  ...   1.000000   1.527525   \n",
       " 566  18.000000  10.000000  60.000000  55.000000  ...   2.886751   5.196152   \n",
       " 347   0.000000  14.000000  24.000000  23.000000  ...   3.214550   3.511885   \n",
       " 584   4.119523  15.303116  28.513717  27.973969  ...   1.698778   3.214550   \n",
       " 136   8.000000  18.000000  37.000000  36.000000  ...   0.577350  12.204347   \n",
       " 581   1.718669  16.374255  36.634215  29.579940  ...   5.705647   7.549834   \n",
       " 68    1.000000  17.000000  32.000000  25.000000  ...   7.505553   1.527525   \n",
       " 107  10.000000  16.000000  40.000000  36.000000  ...   3.511885   2.000000   \n",
       " 615   6.590459  16.709030  38.894050  35.946750  ...   7.128932  10.785793   \n",
       " 272   6.000000  15.000000  33.000000  31.000000  ...   7.000000   6.082763   \n",
       " 204   7.880000  15.000000  38.950000  23.000000  ...   9.208737   7.499230   \n",
       " 690   3.497111  15.836500  26.865095  24.544762  ...   1.179266   1.000000   \n",
       " 550   0.000000  15.000000  27.000000  20.000000  ...   5.131601   2.645751   \n",
       " 583   4.726839  15.276167  33.034786  34.092249  ...   4.777722   7.571878   \n",
       " 613   9.497727  15.590539  33.996020  35.559767  ...   5.505106   2.516611   \n",
       " 456   7.000000  21.000000  26.000000  31.000000  ...   7.094599   9.643651   \n",
       " 66    1.000000  14.000000  31.000000  36.000000  ...   5.507571   3.511885   \n",
       " 144  11.250000  16.000000  41.000000  35.000000  ...   4.582576   6.719440   \n",
       " 201  21.000000  12.000000  53.000000  67.000000  ...   7.810250  12.732334   \n",
       " 645  10.634999  17.778510  38.152022  36.218472  ...   6.906963   5.131601   \n",
       " 521   3.000000  14.000000  29.000000  22.000000  ...   3.511885   8.717798   \n",
       " 255   1.000000  15.000000  26.000000  22.000000  ...   2.309401   5.196152   \n",
       " 506   1.000000  19.000000  27.000000  26.000000  ...   2.645751   2.886751   \n",
       " 50    3.000000  17.000000  26.000000  36.000000  ...   8.082904  11.135529   \n",
       " 111   8.000000  11.000000  33.000000  36.000000  ...   6.806859   2.466441   \n",
       " 88    4.000000  13.000000  27.000000  35.000000  ...   9.018500  13.650397   \n",
       " 402  11.000000  14.000000  50.000000  38.000000  ...   8.326664   4.509250   \n",
       " 125   8.000000  13.000000  47.000000  51.000000  ...   8.326664   9.073772   \n",
       " 529   0.000000  16.000000  31.000000  28.000000  ...   7.371115   4.582576   \n",
       " 249  11.000000  13.000000  32.630000  35.000000  ...   5.789269   5.567764   \n",
       " ..         ...        ...        ...        ...  ...        ...        ...   \n",
       " 246   5.630000  13.000000  40.000000  41.000000  ...  13.000000  10.440307   \n",
       " 30    5.000000  16.800000  27.000000  27.000000  ...   1.732051   7.371115   \n",
       " 24    4.000000  14.000000  38.000000  37.000000  ...  11.269428   9.865766   \n",
       " 422   3.000000  14.000000  30.000000  27.000000  ...   6.806859   7.000000   \n",
       " 618   4.444064  15.585267  30.324690  28.372885  ...   2.512281   3.464102   \n",
       " 385   7.000000  15.000000  47.000000  34.000000  ...   7.505553   2.996448   \n",
       " 206   5.458966  18.490071  33.833480  38.950000  ...  11.484390   8.621678   \n",
       " 375  14.000000  19.000000  43.000000  47.000000  ...   3.511885   7.505553   \n",
       " 134   7.000000  17.000000  31.000000  34.000000  ...   3.511885   5.567764   \n",
       " 314   0.000000  12.000000  28.000000  25.000000  ...   5.686241   4.000000   \n",
       " 553   5.000000  16.000000  27.000000  21.000000  ...   3.464102  14.106736   \n",
       " 525   1.000000  17.000000  27.000000  27.000000  ...   5.773503   2.886751   \n",
       " 162  18.000000  19.000000  43.000000  46.000000  ...   6.806859   5.507571   \n",
       " 45    8.000000  16.000000  35.000000  29.000000  ...   4.582576   7.371115   \n",
       " 366   9.000000  12.000000  37.000000  43.000000  ...   6.000000  12.984852   \n",
       " 561  11.000000  17.000000  46.000000  27.000000  ...   9.539392  10.016653   \n",
       " 301   8.000000  19.000000  34.000000  33.000000  ...   8.386497  11.590226   \n",
       " 530   6.000000  12.000000  26.000000  24.000000  ...   2.000000   1.527525   \n",
       " 109   5.000000  14.000000  26.000000  24.000000  ...   3.605551   3.055050   \n",
       " 91   11.000000  19.000000  40.000000  36.000000  ...   2.309401   7.018093   \n",
       " 522   3.000000  12.000000  23.000000  21.000000  ...   3.055050   3.605551   \n",
       " 703   5.955330  16.185250  36.678898  32.144758  ...   5.707451  10.692677   \n",
       " 428   4.000000  21.000000  32.000000  31.000000  ...   1.527525   4.163332   \n",
       " 384   8.000000  16.000000  29.000000  27.000000  ...   2.000000  20.305993   \n",
       " 411   0.000000  16.000000  29.000000  29.000000  ...   6.928203  10.115994   \n",
       " 620   6.763948  13.254510  32.865145  30.300750  ...   3.826109   2.081666   \n",
       " 163   7.000000  13.000000  42.000000  35.000000  ...  11.239810   6.244998   \n",
       " 480   5.000000  14.000000  27.000000  28.000000  ...   2.081666   5.507571   \n",
       " 278   2.000000  11.000000  27.000000  21.000000  ...   3.464102  12.662280   \n",
       " 503   1.000000  13.000000  35.000000  31.000000  ...   5.567764   4.358899   \n",
       " \n",
       "      std_pos_coping_t1  std_neg_coping_t1  std_pos_coping_t2  \\\n",
       " 524           2.223611           1.000000           2.590581   \n",
       " 566           0.843274           3.000000           1.490712   \n",
       " 347           1.337494           0.000000           1.837873   \n",
       " 584           1.645385           0.602752           2.165839   \n",
       " 136           1.852926           1.500000           1.643790   \n",
       " 581           1.473916           0.851366           1.066270   \n",
       " 68            0.872717           1.597245           1.471988   \n",
       " 107           2.065591           0.500000           1.779513   \n",
       " 615           1.436934           1.004824           1.593113   \n",
       " 272           1.354006           0.500000           1.549193   \n",
       " 204           0.707107           1.914854           0.534221   \n",
       " 690           1.325696           0.488246           1.917298   \n",
       " 550           1.475730           1.000000           1.888562   \n",
       " 583           1.425415           0.629358           1.776765   \n",
       " 613           1.366716           1.624569           1.337206   \n",
       " 456           2.162817           0.500000           2.485514   \n",
       " 66            2.867442           1.000000           1.791957   \n",
       " 144           1.932184           2.645751           1.707423   \n",
       " 201           2.626785           2.872281           2.190592   \n",
       " 645           1.375427           0.996680           2.267425   \n",
       " 521           2.162817           2.362908           1.911951   \n",
       " 255           1.946507           2.000000           0.966092   \n",
       " 506           2.223611           0.500000           2.097618   \n",
       " 50            1.969207           1.000000           1.595131   \n",
       " 111           1.080123           1.154701           1.135292   \n",
       " 88            1.686548           1.500000           1.449138   \n",
       " 402           1.449138           0.957427           1.702939   \n",
       " 125           2.002776           3.000000           2.121320   \n",
       " 529           1.837873           2.000000           1.433721   \n",
       " 249           2.424413           3.000000           2.573368   \n",
       " ..                 ...                ...                ...   \n",
       " 246           2.131770           1.414214           1.649916   \n",
       " 30            1.828782           2.000000           2.078995   \n",
       " 24            2.024846           1.000000           2.250926   \n",
       " 422           1.577621           1.414214           2.221111   \n",
       " 618           1.615773           1.191764           2.365356   \n",
       " 385           0.875595           0.577350           0.674949   \n",
       " 206           1.178511           2.061553           1.828782   \n",
       " 375           1.955050           0.816497           1.957890   \n",
       " 134           2.321398           2.000000           2.451757   \n",
       " 314           0.849837           2.872281           2.601282   \n",
       " 553           1.932184           2.000000           1.712698   \n",
       " 525           1.897367           1.500000           1.636392   \n",
       " 162           1.577621           1.414214           1.619328   \n",
       " 45            1.776388           1.500000           1.286684   \n",
       " 366           0.632456           0.000000           1.364139   \n",
       " 561           1.813529           2.581989           1.988858   \n",
       " 301           1.636392           2.061553           1.316561   \n",
       " 530           2.601282           1.000000           2.260777   \n",
       " 109           1.932184           3.000000           1.813529   \n",
       " 91            1.619328           0.816497           1.449138   \n",
       " 522           2.366432           1.500000           2.282786   \n",
       " 703           1.704219           1.468523           2.194802   \n",
       " 428           1.398412           1.914854           1.969207   \n",
       " 384           0.966092           1.414214           1.494434   \n",
       " 411           2.054805           0.500000           2.002776   \n",
       " 620           1.430873           1.841869           2.371456   \n",
       " 163           2.057507           1.414214           1.702939   \n",
       " 480           1.885618           0.000000           2.043961   \n",
       " 278           1.582039           2.302322           2.073159   \n",
       " 503           1.354006           1.914854           1.505545   \n",
       " \n",
       "      std_neg_coping_t2  intrusion_cutoff  hypertention_cutoff  \\\n",
       " 524           0.000000               0.0                  0.0   \n",
       " 566           2.872281               1.0                  1.0   \n",
       " 347           0.000000               0.0                  0.0   \n",
       " 584           0.000000               0.0                  0.0   \n",
       " 136           1.177409               1.0                  1.0   \n",
       " 581           1.632993               1.0                  1.0   \n",
       " 68            2.061553               1.0                  0.0   \n",
       " 107           1.154701               0.0                  0.0   \n",
       " 615           1.000000               0.0                  1.0   \n",
       " 272           0.577350               0.0                  0.0   \n",
       " 204           1.072314               1.0                  0.0   \n",
       " 690           1.000000               0.0                  0.0   \n",
       " 550           1.500000               0.0                  0.0   \n",
       " 583           1.000000               0.0                  0.0   \n",
       " 613           1.500000               0.0                  0.0   \n",
       " 456           1.500000               0.0                  0.0   \n",
       " 66            2.362908               0.0                  0.0   \n",
       " 144           1.530083               1.0                  1.0   \n",
       " 201           1.408802               1.0                  1.0   \n",
       " 645           1.000000               0.0                  0.0   \n",
       " 521           1.500000               0.0                  0.0   \n",
       " 255           1.000000               0.0                  0.0   \n",
       " 506           0.500000               0.0                  1.0   \n",
       " 50            1.000000               0.0                  1.0   \n",
       " 111           1.000000               0.0                  0.0   \n",
       " 88            2.000000               0.0                  0.0   \n",
       " 402           2.872281               1.0                  1.0   \n",
       " 125           2.000000               0.0                  0.0   \n",
       " 529           2.000000               0.0                  0.0   \n",
       " 249           1.500000               1.0                  1.0   \n",
       " ..                 ...               ...                  ...   \n",
       " 246           0.500000               0.0                  0.0   \n",
       " 30            1.500000               1.0                  0.0   \n",
       " 24            2.500000               0.0                  0.0   \n",
       " 422           1.500000               0.0                  0.0   \n",
       " 618           1.000000               0.0                  0.0   \n",
       " 385           3.000000               1.0                  1.0   \n",
       " 206           1.000000               0.0                  0.0   \n",
       " 375           1.707825               1.0                  1.0   \n",
       " 134           2.000000               0.0                  1.0   \n",
       " 314           0.000000               0.0                  0.0   \n",
       " 553           2.000000               0.0                  0.0   \n",
       " 525           1.500000               0.0                  0.0   \n",
       " 162           2.362908               1.0                  0.0   \n",
       " 45            2.000000               0.0                  0.0   \n",
       " 366           0.629728               0.0                  1.0   \n",
       " 561           2.500000               0.0                  0.0   \n",
       " 301           1.000000               0.0                  0.0   \n",
       " 530           1.000000               0.0                  0.0   \n",
       " 109           2.000000               0.0                  0.0   \n",
       " 91            1.154701               0.0                  0.0   \n",
       " 522           1.000000               0.0                  0.0   \n",
       " 703           2.500000               0.0                  0.0   \n",
       " 428           0.957427               0.0                  0.0   \n",
       " 384           1.414214               1.0                  0.0   \n",
       " 411           2.160247               0.0                  0.0   \n",
       " 620           2.872281               0.0                  0.0   \n",
       " 163           1.290994               1.0                  1.0   \n",
       " 480           1.414214               0.0                  0.0   \n",
       " 278           1.892969               0.0                  0.0   \n",
       " 503           2.362908               1.0                  1.0   \n",
       " \n",
       "      avoidance_cutoff  depression_cutoff  \n",
       " 524               0.0                0.0  \n",
       " 566               0.0                0.0  \n",
       " 347               1.0                1.0  \n",
       " 584               0.0                0.0  \n",
       " 136               1.0                1.0  \n",
       " 581               0.0                0.0  \n",
       " 68                0.0                0.0  \n",
       " 107               0.0                0.0  \n",
       " 615               0.0                0.0  \n",
       " 272               0.0                0.0  \n",
       " 204               0.0                0.0  \n",
       " 690               0.0                0.0  \n",
       " 550               0.0                0.0  \n",
       " 583               0.0                0.0  \n",
       " 613               0.0                0.0  \n",
       " 456               0.0                0.0  \n",
       " 66                0.0                0.0  \n",
       " 144               1.0                1.0  \n",
       " 201               1.0                1.0  \n",
       " 645               0.0                0.0  \n",
       " 521               0.0                0.0  \n",
       " 255               0.0                0.0  \n",
       " 506               0.0                0.0  \n",
       " 50                0.0                1.0  \n",
       " 111               0.0                0.0  \n",
       " 88                0.0                0.0  \n",
       " 402               0.0                1.0  \n",
       " 125               0.0                0.0  \n",
       " 529               0.0                0.0  \n",
       " 249               1.0                1.0  \n",
       " ..                ...                ...  \n",
       " 246               0.0                0.0  \n",
       " 30                0.0                0.0  \n",
       " 24                0.0                0.0  \n",
       " 422               0.0                0.0  \n",
       " 618               0.0                0.0  \n",
       " 385               1.0                1.0  \n",
       " 206               0.0                0.0  \n",
       " 375               1.0                1.0  \n",
       " 134               0.0                0.0  \n",
       " 314               0.0                0.0  \n",
       " 553               0.0                0.0  \n",
       " 525               0.0                0.0  \n",
       " 162               0.0                0.0  \n",
       " 45                0.0                0.0  \n",
       " 366               0.0                0.0  \n",
       " 561               0.0                0.0  \n",
       " 301               0.0                0.0  \n",
       " 530               0.0                0.0  \n",
       " 109               1.0                1.0  \n",
       " 91                0.0                0.0  \n",
       " 522               0.0                0.0  \n",
       " 703               0.0                0.0  \n",
       " 428               0.0                0.0  \n",
       " 384               1.0                1.0  \n",
       " 411               0.0                1.0  \n",
       " 620               0.0                0.0  \n",
       " 163               1.0                1.0  \n",
       " 480               0.0                0.0  \n",
       " 278               0.0                0.0  \n",
       " 503               0.0                0.0  \n",
       " \n",
       " [177 rows x 162 columns],\n",
       " 174    0.0\n",
       " 459    0.0\n",
       " 635    0.0\n",
       " 78     0.0\n",
       " 548    0.0\n",
       " 358    0.0\n",
       " 339    1.0\n",
       " 490    0.0\n",
       " 421    0.0\n",
       " 282    0.0\n",
       " 89     0.0\n",
       " 357    0.0\n",
       " 62     0.0\n",
       " 251    0.0\n",
       " 495    0.0\n",
       " 185    0.0\n",
       " 151    0.0\n",
       " 44     0.0\n",
       " 614    0.0\n",
       " 394    0.0\n",
       " 406    0.0\n",
       " 112    0.0\n",
       " 149    1.0\n",
       " 235    0.0\n",
       " 242    0.0\n",
       " 694    1.0\n",
       " 307    0.0\n",
       " 554    0.0\n",
       " 341    0.0\n",
       " 265    0.0\n",
       "       ... \n",
       " 175    0.0\n",
       " 74     0.0\n",
       " 237    0.0\n",
       " 211    0.0\n",
       " 695    0.0\n",
       " 234    0.0\n",
       " 120    0.0\n",
       " 493    0.0\n",
       " 432    0.0\n",
       " 688    0.0\n",
       " 453    0.0\n",
       " 189    0.0\n",
       " 328    0.0\n",
       " 381    0.0\n",
       " 97     0.0\n",
       " 569    0.0\n",
       " 678    0.0\n",
       " 70     0.0\n",
       " 172    0.0\n",
       " 87     0.0\n",
       " 116    0.0\n",
       " 291    0.0\n",
       " 12     0.0\n",
       " 63     0.0\n",
       " 371    0.0\n",
       " 578    0.0\n",
       " 576    0.0\n",
       " 455    0.0\n",
       " 543    0.0\n",
       " 137    0.0\n",
       " Name: PCL_Strict3, Length: 528, dtype: float64,\n",
       " 524    0.0\n",
       " 566    0.0\n",
       " 347    0.0\n",
       " 584    0.0\n",
       " 136    1.0\n",
       " 581    0.0\n",
       " 68     0.0\n",
       " 107    0.0\n",
       " 615    0.0\n",
       " 272    0.0\n",
       " 204    0.0\n",
       " 690    0.0\n",
       " 550    0.0\n",
       " 583    0.0\n",
       " 613    0.0\n",
       " 456    0.0\n",
       " 66     0.0\n",
       " 144    1.0\n",
       " 201    1.0\n",
       " 645    0.0\n",
       " 521    0.0\n",
       " 255    0.0\n",
       " 506    0.0\n",
       " 50     0.0\n",
       " 111    0.0\n",
       " 88     0.0\n",
       " 402    0.0\n",
       " 125    0.0\n",
       " 529    0.0\n",
       " 249    1.0\n",
       "       ... \n",
       " 246    0.0\n",
       " 30     0.0\n",
       " 24     0.0\n",
       " 422    0.0\n",
       " 618    0.0\n",
       " 385    1.0\n",
       " 206    0.0\n",
       " 375    1.0\n",
       " 134    0.0\n",
       " 314    0.0\n",
       " 553    0.0\n",
       " 525    0.0\n",
       " 162    0.0\n",
       " 45     0.0\n",
       " 366    0.0\n",
       " 561    0.0\n",
       " 301    0.0\n",
       " 530    0.0\n",
       " 109    0.0\n",
       " 91     0.0\n",
       " 522    0.0\n",
       " 703    0.0\n",
       " 428    0.0\n",
       " 384    0.0\n",
       " 411    0.0\n",
       " 620    0.0\n",
       " 163    1.0\n",
       " 480    0.0\n",
       " 278    0.0\n",
       " 503    0.0\n",
       " Name: PCL_Strict3, Length: 177, dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_out, Y, y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.88\n",
      "p 0.7857142857142857\n",
      "r 1.0\n",
      "auc 0.9909638554216867\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = [X[i] for i in mtl_targets] + [Y]\n",
    "X.drop(mtl_targets, axis=1, inplace=True)    \n",
    "y_pred_1 = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X, (t[0]).values.astype(int)).predict_proba(X_out)\n",
    "y_pred_2 = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X, (t[1]).values.astype(int)).predict_proba(X_out)\n",
    "y_pred_3 = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X, (t[2]).values.astype(int)).predict_proba(X_out)\n",
    "y_pred_target = CatBoostClassifier(verbose=0, loss_function='CrossEntropy', n_estimators=100).fit(X, t[3].values.astype(int)).predict_proba(X_out)\n",
    "\n",
    "y_pred = (y_pred_1[:, 1] * y_pred_2[:, 1] *  y_pred_3[:, 1] * y_pred_target[:, 1]) > 0.0625 #+ 0.5 * y_pred_target[:, 1]) > 1\n",
    "\n",
    "print(\"f1\", f1_score(y_out.astype(bool), y_pred))\n",
    "print(\"p\",precision_score(y_out.astype(bool), y_pred))\n",
    "print(\"r\",recall_score(y_out.astype(bool), y_pred))\n",
    "print(\"auc\", roc_auc_score(y_out.astype(bool), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

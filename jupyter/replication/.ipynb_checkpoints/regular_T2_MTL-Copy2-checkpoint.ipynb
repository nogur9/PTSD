{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, roc_auc_score, make_scorer, f1_score, recall_score, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from fancyimpute import IterativeImputer\n",
    "os.environ['PYTHONHASHSEED']=str(271828)\n",
    "random.seed(271828)\n",
    "np.random.seed(271828)\n",
    "#import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(271828)\n",
    "ops.reset_default_graph()\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization\n",
    "#from tensorflow.keras.layers import Conv2D, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import keras.backend as K\n",
    "#session_conf=tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) \n",
    "#sesCriticalSection=tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "#K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PCL_calculator(df):\n",
    "\n",
    "    symptomatic_cutoff = 2\n",
    "\n",
    "    intrusion = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH', 'q6.4_UPSET', 'q6.5_PHYS']\n",
    "    avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES', 'q6.9_DISINT', 'q6.10_DTACH',\n",
    "                 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "    tred = ['q6.1_INTRU', 'q6.2_DREAM', 'q6.3_FLASH']\n",
    "    only_avoidance = ['q6.6_AVTHT', 'q6.7_AVSIT', 'q6.8_AMNES']\n",
    "\n",
    "    hypertension = ['q6.13_SLEEP', 'q6.14_ANGER', 'q6.15_CONC', 'q6.16_HYPER', 'q6.17_STRTL']\n",
    "\n",
    "    depression = ['q6.9_DISINT', 'q6.10_DTACH', 'q6.11_NUMB', 'q6.12_FUTRE']\n",
    "\n",
    "    df[intrusion + avoidance + hypertension].fillna(df[intrusion + avoidance + hypertension].mean(axis=1))\n",
    "    intrusion_cuoff = 1\n",
    "    avoidance_cuoff = 3\n",
    "    hypertension_cuoff = 2\n",
    "    only_avoidance_cutoff = 1\n",
    "    depression_cutoff = 2\n",
    "    tred_cutoff = 1\n",
    "\n",
    "    df['sum'] = (df[intrusion + avoidance + hypertension]).sum(axis=1)\n",
    "\n",
    "    df['intrusion'] = (df[intrusion] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['intrusion_cutoff'] = (df['intrusion'] >= intrusion_cuoff).astype(int)\n",
    "\n",
    "    df['avoidance'] = (df[avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['avoidance_cutoff'] = (df['avoidance'] >= avoidance_cuoff).astype(int)\n",
    "\n",
    "    df['depression'] = (df[depression] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['depression_cutoff'] = df['depression'] >= depression_cutoff\n",
    "\n",
    "    df['hypertention'] = (df[hypertension] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['hypertention_cutoff'] = (df['hypertention'] >= hypertension_cuoff).astype(int)\n",
    "\n",
    "    df['tred'] = (df[tred] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['tred_cutoff'] = df['tred'] >= tred_cutoff\n",
    "\n",
    "    df['only_avoidance'] = (df[only_avoidance] > symptomatic_cutoff).sum(axis=1)\n",
    "    df['only_avoidance_cutoff'] = df['only_avoidance'] >= only_avoidance_cutoff\n",
    "\n",
    "    df['regression_cutoff_33'] = df['sum'] >= 33\n",
    "    df['regression_cutoff_50'] = df['sum'] >= 50\n",
    "    df['diagnosis'] = ((df['hypertention_cutoff']) & (df['avoidance_cutoff']) & (df['intrusion_cutoff']) & (df['sum'] >= 50))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\‏‏PycharmProjects\\PTSD\\Data\\PTSD.xlsx\"\n",
    "df = pd.read_excel(path)\n",
    "#combine with specifics of answers\n",
    "df_pcl2 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL2.xlsx\")\n",
    "df_pcl1 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL1.xlsx\")\n",
    "df = df.merge(df_pcl1, on=\"ID\", how='outer')\n",
    "df = df.merge(df_pcl2, suffixes=('_pcl1', '_pcl2'), on=\"ID\", how='outer')\n",
    "\n",
    "df_pcl3 = pd.read_excel(\"C:\\‏‏PycharmProjects\\PTSD\\Data\\questionnaire6PCL3.xlsx\")\n",
    "df_pcl3 = PCL_calculator(df_pcl3)\n",
    "df = df.merge(df_pcl3.drop(['PCL3_Strict', 'pcl3', 'PCL3_Broad'], axis=1), on=\"ID\", how='outer')\n",
    "# rmoving missing Y's\n",
    "df = df[~df['PCL_Strict3'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\"highschool_diploma\", \"dyslexia\", \"ADHD\", \"T1Acc1t\", \"T1Acc1n\", \"T1bias\", \"phq1\", \"lot1\",\n",
    "                \"trait1\",\n",
    "                \"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\", \"phq2\", \"lot2\", \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\",\n",
    "                \"PCL_Strict2\", \"cd_risc1\", \"active_coping1\", \"planning1\", \"positive_reframing1\", \"acceptance1\",\n",
    "                \"humor1\",\n",
    "                \"religion1\", \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\", \"active_coping2\", \"planning2\",\n",
    "                \"positive_reframing2\", \"acceptance2\", \"humor2\", \"religion2\", \"emotional_support2\",\n",
    "                \"instrumental_support2\",\n",
    "                \"self_distraction2\", \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\",\n",
    "                \"self_blame2\",\n",
    "                \"trauma_history8_1\", \"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\", \"COMT_Hap1_recode\",\n",
    "                \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\", 'q6.1_INTRU_pcl1', \n",
    "                'q6.2_DREAM_pcl1', 'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                  'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                  'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                  'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1',\n",
    "                  'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2', 'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2',\n",
    "                  'q6.5_PHYS_pcl2', 'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                  'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                  'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_features = [\"age\", \"highschool_diploma\", \"dyslexia\", \"ADHD\", \"phq1\", \"lot1\",\n",
    "                    \"trait1\", \"state1\", \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\", \"phq2\", \"lot2\",\n",
    "                    \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\", \"PCL_Strict2\", \"cd_risc1\", \"active_coping1\",\n",
    "                    \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\", \"religion1\",\n",
    "                    \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                    \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\", \"active_coping2\",\n",
    "                    \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\", \"religion2\", \"emotional_support2\",\n",
    "                    \"instrumental_support2\", \"self_distraction2\", \"denial2\", \"venting2\", \"substance_use2\",\n",
    "                    \"behavioral_disengagement2\", \"self_blame2\", \"trauma_history8_1\",\n",
    "                    'q6.1_INTRU_pcl1', 'q6.2_DREAM_pcl1', 'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                    'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                    'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                    'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1',\n",
    "                    'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2', 'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2',\n",
    "                    'q6.5_PHYS_pcl2', 'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                    'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                    'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2',\n",
    "                    'intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "genome_features = [\"HML_5HTT\", \"HL_MAOA\", \"HML_NPY\", \"COMT_Hap1_recode\",\n",
    "                \"COMT_Hap2_recode\", \"COMT_Hap1_LvsMH\", \"HML_FKBP5\"]\n",
    "continuous_features = [\"T1Acc1t\", \"T1Acc1n\", \"T1bias\"]\n",
    "t2_features = [\n",
    "    \"lot2\", \"trait2\", \"state2\", \"PCL2\", \"PCL_Broad2\", \"PCL_Strict2\",\n",
    "    \"active_coping2\", \"planning2\", \"positive_reframing2\", \"acceptance2\", \"humor2\", \n",
    "    \"religion2\", \"emotional_support2\", \"instrumental_support2\", \"self_distraction2\",\n",
    "    \"denial2\", \"venting2\", \"substance_use2\", \"behavioral_disengagement2\", \"self_blame2\",\n",
    "    'q6.1_INTRU_pcl2', 'q6.2_DREAM_pcl2', 'q6.3_FLASH_pcl2', 'q6.4_UPSET_pcl2', 'q6.5_PHYS_pcl2',\n",
    "    'q6.6_AVTHT_pcl2', 'q6.7_AVSIT_pcl2', 'q6.8_AMNES_pcl2', 'q6.9_DISINT_pcl2',\n",
    "                    'q6.10_DTACH_pcl2', 'q6.11_NUMB_pcl2', 'q6.12_FUTRE_pcl2', 'q6.13_SLEEP_pcl2',\n",
    "                    'q6.14_ANGER_pcl2', 'q6.15_CONC_pcl2', 'q6.16_HYPER_pcl2', 'q6.17_STRTL_pcl2'\n",
    "]\n",
    "t1_features =[ \"active_coping1\",\"T1Acc1t\", \"T1Acc1n\", \"T1bias\", \"phq1\", \"lot1\",\n",
    "                \"trait1\",\"state1\",\n",
    "                    \"planning1\", \"positive_reframing1\", \"acceptance1\", \"humor1\", \"religion1\",\n",
    "                    \"emotional_support1\", \"instrumental_support1\", \"self_distraction1\", \"denial1\",\n",
    "                    \"venting1\", \"substance_use1\", \"behavioral_disengagement1\", \"self_blame1\",\n",
    "              \"PCL1\", \"PCL_Broad1\", \"PCL_Strict1\",\"cd_risc1\", \n",
    "                'q6.1_INTRU_pcl1', 'q6.2_DREAM_pcl1', 'q6.3_FLASH_pcl1', 'q6.4_UPSET_pcl1',\n",
    "                    'q6.5_PHYS_pcl1', 'q6.6_AVTHT_pcl1', 'q6.7_AVSIT_pcl1', 'q6.8_AMNES_pcl1', 'q6.9_DISINT_pcl1',\n",
    "                    'q6.10_DTACH_pcl1', 'q6.11_NUMB_pcl1','q6.13_SLEEP_pcl1', 'q6.12_FUTRE_pcl1', 'q6.13_SLEEP_pcl1',\n",
    "                    'q6.14_ANGER_pcl1', 'q6.15_CONC_pcl1', 'q6.16_HYPER_pcl1', 'q6.17_STRTL_pcl1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[features + ['PCL_Strict3', 'intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50']]\n",
    "#df1 = df1.dropna(thresh=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[features + ['PCL_Strict3', 'intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50']]\n",
    "#df1 = df1.dropna(thresh=47)\n",
    "df_t1 = df1[t1_features]\n",
    "df_t2 = df1[t2_features]\n",
    "df1 = df1[df_t2.isna().astype(int).sum(axis=1)<36]\n",
    "df1 = df1[df_t1.isna().astype(int).sum(axis=1)<42]\n",
    "mice = IterativeImputer()\n",
    "df1 = pd.DataFrame(mice.fit_transform(df1), columns=df1.columns)\n",
    "df1 = df1[(np.abs(stats.zscore(df1)) > 3).sum(axis=1)<15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## target imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCL_Strict3             18.629630\n",
       "intrusion_cutoff         3.568966\n",
       "avoidance_cutoff         6.260274\n",
       "hypertention_cutoff      3.732143\n",
       "regression_cutoff_50    19.384615\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = df1[['PCL_Strict3', 'intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50']]\n",
    "(1 - dt.sum()/dt.count())/(dt.sum()/dt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cut off the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, X_out, Y, y_out = train_test_split(df1[features + ['intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50']], df1['PCL_Strict3'],\n",
    "                                      test_size=0.25, random_state=271828, stratify=df1['PCL_Strict3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, stratify=Y, random_state=271828)\n",
    "cv = StratifiedKFold(5, random_state=271828)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV model of roc auc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1= 170 layer2= 100 epochs= 200 activation= elu batch= 1 dr= 0.5 loss_weight= 1 layer3= 10\n"
     ]
    }
   ],
   "source": [
    "for loss_weight in [1, 2, 0.5, 0.1]:\n",
    "    for epochs in [200, 400]:\n",
    "        for layer3 in [10, 0]:\n",
    "            for layer2 in [100, 50, 0]:\n",
    "                layer1 = 170\n",
    "                dr = 0.5\n",
    "                activation = 'elu'\n",
    "                batch = 1\n",
    "                \n",
    "                print(\"layer1=\", layer1,\"layer2=\", layer2, \"epochs=\", epochs, \"activation=\", activation,\n",
    "                     \"batch=\",batch, \"dr=\", dr, \"loss_weight=\",loss_weight, \"layer3=\", layer3)\n",
    "                \n",
    "                scores_f = []\n",
    "                scores_p = []\n",
    "                scores_r = []\n",
    "                scores_auc = []\n",
    "                \n",
    "                for train, test in cv.split(X_train, y_train):\n",
    "                    X_train_cv = X_train.iloc[train]\n",
    "                    y_train_cv = y_train.iloc[train]\n",
    "\n",
    "                    visible = Input(shape=(91,))\n",
    "                    x = Dense(layer1, activation=activation)(visible)\n",
    "                    if batch:\n",
    "                        x = BatchNormalization()(x)\n",
    "                   \n",
    "                    \n",
    "                    x = Dropout(dr)(x)\n",
    "                    if layer2 > 0:\n",
    "                        x = Dense(layer2, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "\n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "\n",
    "                    if layer3 > 0:\n",
    "                        x = Dense(layer3, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "                        \n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "                    output_intrusion = Dense(1, activation='sigmoid')(x)\n",
    "                    output_avoid = Dense(1, activation='sigmoid')(x)\n",
    "                    output_hype = Dense(1, activation='sigmoid')(x)\n",
    "                    output_50 = Dense(1, activation='sigmoid')(x)\n",
    "                    output_diagnosis = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "                    model = Model(inputs=visible, outputs=[output_intrusion, output_avoid, output_hype, output_50, output_diagnosis])\n",
    "\n",
    "                    model.compile(loss=[\"binary_crossentropy\"] * 5,\n",
    "                              optimizer='adam', loss_weights = [loss_weight] * 4 + [1])\n",
    "                    \n",
    "                    t = [X_train_cv['intrusion_cutoff'], X_train_cv['avoidance_cutoff'],X_train_cv['hypertention_cutoff'],\n",
    "                         X_train_cv['regression_cutoff_50'], y_train_cv]\n",
    "                    \n",
    "                    X_train_cv.drop(['intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50'], axis=1, inplace=True)\n",
    "                    \n",
    "                    class_weight = [{0:1, 1:4}, {0:1, 1:6}, {0:1, 1:4}, {0:1, 1:20}, {0:1, 1:19}]\n",
    "                    \n",
    "                \n",
    "                    \n",
    "                    model.fit(X_train_cv, t, epochs=epochs, verbose=0, class_weight=class_weight)\n",
    "                    \n",
    "                    y_pred = model.predict(X_train[features].iloc[test])[4] >= 0.5\n",
    "\n",
    "                    scores_f.append(f1_score(y_train.iloc[test], y_pred))\n",
    "                    scores_p.append(precision_score(y_train.iloc[test], y_pred))\n",
    "                    scores_r.append(recall_score(y_train.iloc[test], y_pred))\n",
    "                    scores_auc.append(roc_auc_score(y_train.iloc[test], y_pred))\n",
    "                ### results\n",
    "                print(\"scores_f=\",np.mean(scores_f))\n",
    "                #print(\"scores_p=\",scores_p)\n",
    "                #print(\"scores_r=\",scores_r)\n",
    "                #print(\"scores_auc=\",scores_auc)\n",
    "                print(\"mean auc=\",np.mean(scores_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1= 170 layer2= 100 epochs= 400 activation= tanh batch= 1 dr= 0.5 loss_weight= 1 layer3= 10\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-65cd974bc61d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mscores_auc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                     \u001b[0mX_train_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                     \u001b[0my_train_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "for loss_weight in [1, 2, 0.5, 0.1]:\n",
    "    for epochs in [400, 200]:\n",
    "        for layer3 in [10, 0]:\n",
    "            for layer2 in [100, 50, 0]:\n",
    "                layer1 = 170\n",
    "                dr = 0.5\n",
    "                activation = 'tanh'\n",
    "                batch = 1\n",
    "                \n",
    "                print(\"layer1=\", layer1,\"layer2=\", layer2, \"epochs=\", epochs, \"activation=\", activation,\n",
    "                     \"batch=\",batch, \"dr=\", dr, \"loss_weight=\",loss_weight, \"layer3=\", layer3)\n",
    "                \n",
    "                scores_f = []\n",
    "                scores_p = []\n",
    "                scores_r = []\n",
    "                scores_auc = []\n",
    "                \n",
    "                for train, test in cv.split(X_train, y_train):\n",
    "                    X_train_cv = X_train.iloc[train]\n",
    "                    y_train_cv = y_train.iloc[train]\n",
    "\n",
    "                    visible = Input(shape=(91,))\n",
    "                    x = Dense(layer1, activation=activation)(visible)\n",
    "                    if batch:\n",
    "                        x = BatchNormalization()(x)\n",
    "                   \n",
    "                    \n",
    "                    x = Dropout(dr)(x)\n",
    "                    if layer2 > 0:\n",
    "                        x = Dense(layer2, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "\n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "\n",
    "                    if layer3 > 0:\n",
    "                        x = Dense(layer3, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "                        \n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "                    output_intrusion = Dense(1, activation='sigmoid')(x)\n",
    "                    output_avoid = Dense(1, activation='sigmoid')(x)\n",
    "                    output_hype = Dense(1, activation='sigmoid')(x)\n",
    "                    output_50 = Dense(1, activation='sigmoid')(x)\n",
    "                    output_diagnosis = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "                    model = Model(inputs=visible, outputs=[output_intrusion, output_avoid, output_hype, output_50, output_diagnosis])\n",
    "\n",
    "                    model.compile(loss=[\"binary_crossentropy\"] * 5,\n",
    "                              optimizer='adam', loss_weights = [loss_weight] * 4 + [1])\n",
    "                    \n",
    "                    t = [X_train_cv['intrusion_cutoff'], X_train_cv['avoidance_cutoff'],X_train_cv['hypertention_cutoff'],\n",
    "                         X_train_cv['regression_cutoff_50'], y_train_cv]\n",
    "                    \n",
    "                    X_train_cv.drop(['intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50'], axis=1, inplace=True)\n",
    "                    \n",
    "                    class_weight = class_weight = [{0:1, 1:4}, {0:1, 1:6}, {0:1, 1:4}, {0:1, 1:20}, {0:1, 1:19}]\n",
    "                \n",
    "                    \n",
    "                    model.fit(X_train_cv, t, epochs=epochs, verbose=0, class_weight=class_weight)\n",
    "                    \n",
    "                    y_pred = model.predict(X_train[features].iloc[test])[4] >= 0.5\n",
    "\n",
    "                    scores_f.append(f1_score(y_train.iloc[test], y_pred))\n",
    "                    scores_p.append(precision_score(y_train.iloc[test], y_pred))\n",
    "                    scores_r.append(recall_score(y_train.iloc[test], y_pred))\n",
    "                    scores_auc.append(roc_auc_score(y_train.iloc[test], y_pred))\n",
    "                ### results\n",
    "                print(\"scores_f=\",np.mean(scores_f))\n",
    "                #print(\"scores_p=\",scores_p)\n",
    "                #print(\"scores_r=\",scores_r)\n",
    "                #print(\"scores_auc=\",scores_auc)\n",
    "                print(\"mean auc=\",np.mean(scores_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1= 100 layer2= 100 epochs= 400 activation= tanh batch= 1 dr= 0.5 loss_weight= 1 layer3= 10\n",
      "scores_f= 0.27986013986013986\n",
      "mean auc= 0.737437343358396\n",
      "layer1= 100 layer2= 50 epochs= 400 activation= tanh batch= 1 dr= 0.5 loss_weight= 1 layer3= 10\n",
      "scores_f= 0.3367094017094017\n",
      "mean auc= 0.8110902255639096\n",
      "layer1= 100 layer2= 0 epochs= 400 activation= tanh batch= 1 dr= 0.5 loss_weight= 1 layer3= 10\n",
      "scores_f= 0.3815873015873016\n",
      "mean auc= 0.7329573934837093\n",
      "layer1= 100 layer2= 100 epochs= 400 activation= tanh batch= 1 dr= 0.5 loss_weight= 1 layer3= 0\n",
      "scores_f= 0.15777777777777777\n",
      "mean auc= 0.606359649122807\n",
      "layer1= 100 layer2= 50 epochs= 400 activation= tanh batch= 1 dr= 0.5 loss_weight= 1 layer3= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function WeakSet.__init__.<locals>._remove at 0x0000026EADFD9A60>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Anaconda\\lib\\_weakrefset.py\", line 39, in _remove\n",
      "    self = selfref()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "for loss_weight in [1, 2, 0.5, 0.1]:\n",
    "    for epochs in [400, 200]:\n",
    "        for layer3 in [10, 0]:\n",
    "            for layer2 in [100, 50, 0]:\n",
    "                layer1 = 100\n",
    "                dr = 0.5\n",
    "                activation = 'tanh'\n",
    "                batch = 1\n",
    "                \n",
    "                print(\"layer1=\", layer1,\"layer2=\", layer2, \"epochs=\", epochs, \"activation=\", activation,\n",
    "                     \"batch=\",batch, \"dr=\", dr, \"loss_weight=\",loss_weight, \"layer3=\", layer3)\n",
    "                \n",
    "                scores_f = []\n",
    "                scores_p = []\n",
    "                scores_r = []\n",
    "                scores_auc = []\n",
    "                \n",
    "                for train, test in cv.split(X_train, y_train):\n",
    "                    X_train_cv = X_train.iloc[train]\n",
    "                    y_train_cv = y_train.iloc[train]\n",
    "\n",
    "                    visible = Input(shape=(91,))\n",
    "                    x = Dense(layer1, activation=activation)(visible)\n",
    "                    if batch:\n",
    "                        x = BatchNormalization()(x)\n",
    "                   \n",
    "                    \n",
    "                    x = Dropout(dr)(x)\n",
    "                    if layer2 > 0:\n",
    "                        x = Dense(layer2, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "\n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "\n",
    "                    if layer3 > 0:\n",
    "                        x = Dense(layer3, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "                        \n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "                    output_intrusion = Dense(1, activation='sigmoid')(x)\n",
    "                    output_avoid = Dense(1, activation='sigmoid')(x)\n",
    "                    output_hype = Dense(1, activation='sigmoid')(x)\n",
    "                    output_50 = Dense(1, activation='sigmoid')(x)\n",
    "                    output_diagnosis = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "                    model = Model(inputs=visible, outputs=[output_intrusion, output_avoid, output_hype, output_50, output_diagnosis])\n",
    "\n",
    "                    model.compile(loss=[\"binary_crossentropy\"] * 5,\n",
    "                              optimizer='adam', loss_weights = [loss_weight] * 4 + [1])\n",
    "                    \n",
    "                    t = [X_train_cv['intrusion_cutoff'], X_train_cv['avoidance_cutoff'],X_train_cv['hypertention_cutoff'],\n",
    "                         X_train_cv['regression_cutoff_50'], y_train_cv]\n",
    "                    \n",
    "                    X_train_cv.drop(['intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50'], axis=1, inplace=True)\n",
    "                    \n",
    "                    class_weight = class_weight = [{0:1, 1:4}, {0:1, 1:6}, {0:1, 1:4}, {0:1, 1:20}, {0:1, 1:19}]\n",
    "                \n",
    "                    \n",
    "                    model.fit(X_train_cv, t, epochs=epochs, verbose=0, class_weight=class_weight)\n",
    "                    \n",
    "                    y_pred = model.predict(X_train[features].iloc[test])[4] >= 0.5\n",
    "\n",
    "                    scores_f.append(f1_score(y_train.iloc[test], y_pred))\n",
    "                    scores_p.append(precision_score(y_train.iloc[test], y_pred))\n",
    "                    scores_r.append(recall_score(y_train.iloc[test], y_pred))\n",
    "                    scores_auc.append(roc_auc_score(y_train.iloc[test], y_pred))\n",
    "                ### results\n",
    "                print(\"scores_f=\",np.mean(scores_f))\n",
    "                #print(\"scores_p=\",scores_p)\n",
    "                #print(\"scores_r=\",scores_r)\n",
    "                #print(\"scores_auc=\",scores_auc)\n",
    "                print(\"mean auc=\",np.mean(scores_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss_weight in [1, 2, 0.5, 0.1]:\n",
    "    for epochs in [400, 200]:\n",
    "        for layer3 in [10, 0]:\n",
    "            for layer2 in [50, 0]:\n",
    "                layer1 = 50\n",
    "                dr = 0.5\n",
    "                activation = 'tanh'\n",
    "                batch = 1\n",
    "                \n",
    "                print(\"layer1=\", layer1,\"layer2=\", layer2, \"epochs=\", epochs, \"activation=\", activation,\n",
    "                     \"batch=\",batch, \"dr=\", dr, \"loss_weight=\",loss_weight, \"layer3=\", layer3)\n",
    "                \n",
    "                scores_f = []\n",
    "                scores_p = []\n",
    "                scores_r = []\n",
    "                scores_auc = []\n",
    "                \n",
    "                for train, test in cv.split(X_train, y_train):\n",
    "                    X_train_cv = X_train.iloc[train]\n",
    "                    y_train_cv = y_train.iloc[train]\n",
    "\n",
    "                    visible = Input(shape=(91,))\n",
    "                    x = Dense(layer1, activation=activation)(visible)\n",
    "                    if batch:\n",
    "                        x = BatchNormalization()(x)\n",
    "                   \n",
    "                    \n",
    "                    x = Dropout(dr)(x)\n",
    "                    if layer2 > 0:\n",
    "                        x = Dense(layer2, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "\n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "\n",
    "                    if layer3 > 0:\n",
    "                        x = Dense(layer3, activation=activation)(x)\n",
    "                        if batch:\n",
    "                            x = BatchNormalization()(x)\n",
    "                        \n",
    "                        x = Dropout(dr)(x)\n",
    "\n",
    "                    output_intrusion = Dense(1, activation='sigmoid')(x)\n",
    "                    output_avoid = Dense(1, activation='sigmoid')(x)\n",
    "                    output_hype = Dense(1, activation='sigmoid')(x)\n",
    "                    output_50 = Dense(1, activation='sigmoid')(x)\n",
    "                    output_diagnosis = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "                    model = Model(inputs=visible, outputs=[output_intrusion, output_avoid, output_hype, output_50, output_diagnosis])\n",
    "\n",
    "                    model.compile(loss=[\"binary_crossentropy\"] * 5,\n",
    "                              optimizer='adam', loss_weights = [loss_weight] * 4 + [1])\n",
    "                    \n",
    "                    t = [X_train_cv['intrusion_cutoff'], X_train_cv['avoidance_cutoff'],X_train_cv['hypertention_cutoff'],\n",
    "                         X_train_cv['regression_cutoff_50'], y_train_cv]\n",
    "                    \n",
    "                    X_train_cv.drop(['intrusion_cutoff', 'avoidance_cutoff', 'hypertention_cutoff', 'regression_cutoff_50'], axis=1, inplace=True)\n",
    "                    \n",
    "                    class_weight = class_weight = [{0:1, 1:4}, {0:1, 1:6}, {0:1, 1:4}, {0:1, 1:20}, {0:1, 1:19}]\n",
    "                \n",
    "                    \n",
    "                    model.fit(X_train_cv, t, epochs=epochs, verbose=0, class_weight=class_weight)\n",
    "                    \n",
    "                    y_pred = model.predict(X_train[features].iloc[test])[4] >= 0.5\n",
    "\n",
    "                    scores_f.append(f1_score(y_train.iloc[test], y_pred))\n",
    "                    scores_p.append(precision_score(y_train.iloc[test], y_pred))\n",
    "                    scores_r.append(recall_score(y_train.iloc[test], y_pred))\n",
    "                    scores_auc.append(roc_auc_score(y_train.iloc[test], y_pred))\n",
    "                ### results\n",
    "                print(\"scores_f=\",np.mean(scores_f))\n",
    "                #print(\"scores_p=\",scores_p)\n",
    "                #print(\"scores_r=\",scores_r)\n",
    "                #print(\"scores_auc=\",scores_auc)\n",
    "                print(\"mean auc=\",np.mean(scores_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
